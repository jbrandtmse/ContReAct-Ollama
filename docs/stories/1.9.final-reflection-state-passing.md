# Story 1.9: Implement Final Reflection and State Passing

**Status**: Ready

---

## Story

**As a** Researcher,
**I want** the agent's final thought in a cycle to be recognized as its reflection and for its state to persist to the next cycle,
**so that** the experiment is continuous.

---

## Acceptance Criteria

1. When an LLM response does not contain a tool call, the orchestrator identifies it as the "final reflection" for the cycle
2. The `CYCLE_END` log event payload includes the text of the final reflection
3. The complete message history, including the final reflection, is correctly passed as the starting state for the next cycle

---

## Tasks / Subtasks

- [ ] **Task 1: Update CYCLE_END Logging** (AC: 2)
  - [ ] Modify CycleOrchestrator.run_experiment() to include reflection in CYCLE_END payload
  - [ ] Extract final reflection from agent_state.reflection_history
  - [ ] Add metrics to CYCLE_END payload (optional for this story)
  - [ ] Verify log format matches schema

- [ ] **Task 2: Implement State Persistence Between Cycles** (AC: 3)
  - [ ] Update run_experiment() to pass agent_state between cycles
  - [ ] Ensure message_history persists from cycle N to cycle N+1
  - [ ] Ensure reflection_history accumulates across cycles
  - [ ] Do NOT reset message_history between cycles

- [ ] **Task 3: Verify _load_state Logic** (AC: 3)
  - [ ] Review _load_state() implementation
  - [ ] For cycle 1: Initialize new AgentState with empty histories
  - [ ] For cycle 2+: Receive AgentState from previous cycle
  - [ ] Update cycle_number for new cycle
  - [ ] Keep all other state fields intact

- [ ] **Task 4: Update run_experiment Loop** (AC: 1, 2, 3)
  - [ ] Modify loop to pass agent_state between iterations
  - [ ] After cycle N: extract agent_state
  - [ ] Before cycle N+1: pass agent_state to _load_state or _execute_cycle
  - [ ] Ensure state continuity maintained

- [ ] **Task 5: Testing** (AC: 1, 2, 3)
  - [ ] Write unit tests for state passing
  - [ ] Test CYCLE_END payload includes reflection
  - [ ] Integration test: run 2 cycles, verify state persists
  - [ ] Verify message_history grows across cycles
  - [ ] Verify reflection_history accumulates

---

## Dev Notes

### Previous Story Insights
From Story 1.8:
- _execute_cycle() now stores final reflection in agent_state.reflection_history
- ReAct loop correctly identifies final reflection vs tool calls
- Message history updated throughout cycle

### Current Implementation Gap

**In Story 1.8**, _execute_cycle() stores the reflection:
```python
elif response_type == "FINAL_REFLECTION":
    agent_state.reflection_history.append(data)
    break
```

**In Story 1.4**, run_experiment() doesn't pass state between cycles:
```python
for cycle_num in range(1, self.config.cycle_count + 1):
    agent_state = self._load_state(cycle_num)  # Creates NEW state each time
    agent_state = self._execute_cycle(agent_state)
    # State is lost here - not passed to next cycle!
```

**Story 1.9 fixes this** by maintaining state continuity.

### Updated run_experiment() Implementation

**CRITICAL CHANGE**: Pass agent_state between cycles

```python
def run_experiment(self) -> None:
    """
    Main public method executing full experimental run from Cycle 1 to cycle_count.
    
    Iterates through cycles, executing each one and tracking completion.
    Logs CYCLE_START and CYCLE_END events for each cycle.
    Maintains state continuity across cycles.
    """
    print(f"\nStarting experiment: {self.config.run_id}")
    print(f"Model: {self.config.model_name}")
    print(f"Total cycles: {self.config.cycle_count}\n")
    
    # Initialize state for first cycle
    agent_state = None
    
    for cycle_num in range(1, self.config.cycle_count + 1):
        # Log cycle start
        if self.logger:
            self.logger.log_event(
                run_id=self.config.run_id,
                cycle_number=cycle_num,
                event_type=EventType.CYCLE_START,
                payload={}
            )
        
        print(f"Cycle {cycle_num} starting...")
        
        # Load state for this cycle
        if agent_state is None:
            # First cycle: create new state
            agent_state = self._load_state(cycle_num)
        else:
            # Subsequent cycles: update cycle number, keep history
            agent_state.cycle_number = cycle_num
        
        # Execute cycle
        agent_state = self._execute_cycle(agent_state)
        
        print(f"Cycle {cycle_num} finished.")
        
        # Extract final reflection for logging
        final_reflection = agent_state.reflection_history[-1] if agent_state.reflection_history else ""
        
        # Log cycle end with reflection
        if self.logger:
            self.logger.log_event(
                run_id=self.config.run_id,
                cycle_number=cycle_num,
                event_type=EventType.CYCLE_END,
                payload={
                    "final_reflection": final_reflection
                }
            )
    
    print(f"\n✓ Experiment {self.config.run_id} completed successfully")
    print(f"✓ Executed {self.config.cycle_count} cycles")
    print(f"✓ Log file: logs/{self.config.run_id}.jsonl")
```

### CYCLE_END Payload Structure

**Source**: [docs/architecture/data-models.md#logrecord]

Complete CYCLE_END payload (for Story 1.9):
```python
{
    "final_reflection": str  # Agent's reflection text for the cycle
}
```

**For Future Enhancement** (not required for Story 1.9):
```python
{
    "final_reflection": str,
    "metrics": {
        "memory_ops_total": int,
        "messages_to_operator": int,
        "response_chars": int,
        "memory_write_chars": int
    }
}
```

### State Continuity Verification

**Cycle 1**:
- message_history: [] → [system, assistant, ...]
- reflection_history: [] → ["Reflection 1"]

**Cycle 2**:
- message_history: [system, assistant, ...] → [system, assistant, ..., system, assistant, ...]
- reflection_history: ["Reflection 1"] → ["Reflection 1", "Reflection 2"]

**Cycle 3**:
- message_history: continues to grow
- reflection_history: ["Reflection 1", "Reflection 2"] → ["Reflection 1", "Reflection 2", "Reflection 3"]

**Key Principle**: State accumulates, never resets (except for new experiment runs).

### Alternative _load_state Approach

Instead of modifying run_experiment(), could update _load_state():

```python
def _load_state(self, cycle_number: int, previous_state: Optional[AgentState] = None) -> AgentState:
    """
    LOAD_STATE: Load or initialize AgentState.
    
    Args:
        cycle_number: Current cycle number (1-based)
        previous_state: State from previous cycle (None for cycle 1)
        
    Returns:
        AgentState: Initialized or updated state for this cycle
    """
    if previous_state is None:
        # First cycle: create new state
        return AgentState(
            run_id=self.config.run_id,
            cycle_number=cycle_number,
            model_name=self.config.model_name,
            message_history=[],
            reflection_history=[]
        )
    else:
        # Subsequent cycles: update cycle number, preserve history
        previous_state.cycle_number = cycle_number
        return previous_state
```

**Either approach works** - choose based on clarity preference. The Dev Notes show the simpler inline approach in run_experiment().

### Import Organization

No new imports needed for this story - all functionality exists in CycleOrchestrator.

### Coding Standards
**Source**: [docs/architecture/coding-standards.md]

**Type Hints**:
- Use `Optional[AgentState]` if modifying _load_state signature
- Keep existing type hints for run_experiment() (returns None)

**Docstrings**:
- Update run_experiment() docstring to mention state continuity
- Update _load_state() docstring if signature changes

**Critical Rules**:
1. **NEVER reset message_history** between cycles
2. **ALWAYS accumulate reflection_history**
3. **Update cycle_number** for each new cycle
4. **Keep run_id and model_name** constant across cycles

### Example Log Output

After 2 cycles, `logs/run-id.jsonl` should show:

```jsonl
{"timestamp": "...", "run_id": "...", "cycle_number": 1, "event_type": "CYCLE_START", "payload": {}}
{"timestamp": "...", "run_id": "...", "cycle_number": 1, "event_type": "LLM_INVOCATION", "payload": {...}}
{"timestamp": "...", "run_id": "...", "cycle_number": 1, "event_type": "CYCLE_END", "payload": {"final_reflection": "I explored X and learned Y. Next I will Z."}}
{"timestamp": "...", "run_id": "...", "cycle_number": 2, "event_type": "CYCLE_START", "payload": {}}
{"timestamp": "...", "run_id": "...", "cycle_number": 2, "event_type": "LLM_INVOCATION", "payload": {...}}
{"timestamp": "...", "run_id": "...", "cycle_number": 2, "event_type": "CYCLE_END", "payload": {"final_reflection": "Building on previous cycle, I investigated A and found B."}}
```

Note: Cycle 2's LLM_INVOCATION payload.prompt_messages will include Cycle 1's messages.

---

## Testing

**Source**: [docs/architecture/coding-standards.md#8-testing-standards]

### Test Standards for This Story
- **Test Coverage Target**: >80% code coverage
- **Test Framework**: pytest 8.2.2+
- **Test File Locations**: 
  - `tests/unit/test_cycle_orchestrator.py` (update)
  - `tests/integration/test_state_persistence.py`

### Testing Requirements for Story 1.9

**Unit Tests** (`tests/unit/test_cycle_orchestrator.py` - UPDATE):

1. **test_run_experiment_passes_state_between_cycles**
   - Mock _execute_cycle to track agent_state between calls
   - Run experiment with cycle_count=2
   - Assert agent_state from cycle 1 passed to cycle 2
   - Assert cycle_number updated correctly (1, then 2)

2. **test_run_experiment_logs_cycle_end_with_reflection**
   - Mock logger
   - Mock _execute_cycle to add reflection to agent_state
   - Run experiment with 1 cycle
   - Assert CYCLE_END event logged
   - Assert payload contains final_reflection

3. **test_run_experiment_empty_reflection_handled**
   - Mock _execute_cycle to not add reflection
   - Run experiment
   - Assert CYCLE_END still logged
   - Assert reflection is empty string in payload

4. **test_cycle_number_increments_correctly**
   - Run experiment with cycle_count=3
   - Track cycle_number in agent_state across cycles
   - Assert sequence is 1, 2, 3

**Integration Tests** (`tests/integration/test_state_persistence.py`):

1. **test_message_history_persists_across_cycles**
   - Run 2 cycles with real components (mock Ollama responses)
   - After cycle 1: verify message_history has N messages
   - After cycle 2: verify message_history has >N messages
   - Verify cycle 1 messages still present in cycle 2

2. **test_reflection_history_accumulates**
   - Run 3 cycles
   - After cycle 1: assert reflection_history has 1 entry
   - After cycle 2: assert reflection_history has 2 entries
   - After cycle 3: assert reflection_history has 3 entries
   - Verify all reflections preserved

3. **test_agent_receives_own_reflections**
   - Run 2 cycles
   - In cycle 2, verify LLM receives cycle 1's reflection in prompt
   - Check that assistant message from cycle 1 is in message_history

4. **test_cycle_end_log_contains_reflections**
   - Run experiment with 2 cycles
   - Read log file
   - Parse CYCLE_END events
   - Assert each contains different reflection text
   - Verify reflections match what agent produced

**Mock Strategy Example**:

```python
from unittest.mock import Mock, MagicMock, call
import pytest

def test_run_experiment_passes_state_between_cycles():
    # Setup
    mock_config = Mock()
    mock_config.run_id = "test-run"
    mock_config.model_name = "llama3:latest"
    mock_config.cycle_count = 2
    
    mock_ollama = Mock()
    mock_logger = Mock()
    mock_dispatcher = Mock()
    
    orchestrator = CycleOrchestrator(
        config=mock_config,
        ollama_interface=mock_ollama,
        logger=mock_logger,
        tool_dispatcher=mock_dispatcher
    )
    
    # Track agent_state across calls
    states_seen = []
    
    def mock_execute_cycle(state):
        states_seen.append(state)
        # Add a reflection
        state.reflection_history.append(f"Reflection for cycle {state.cycle_number}")
        # Add some messages
        state.message_history.append({"role": "assistant", "content": "test"})
        return state
    
    orchestrator._execute_cycle = mock_execute_cycle
    
    # Run
    orchestrator.run_experiment()
    
    # Verify
    assert len(states_seen) == 2
    
    # Cycle 1: new state
    assert states_seen[0].cycle_number == 1
    assert len(states_seen[0].reflection_history) == 0  # Empty before execute
    
    # Cycle 2: same state object, updated cycle number
    assert states_seen[1].cycle_number == 2
    # Should have cycle 1's reflection
    assert len(states_seen[1].reflection_history) == 1
    assert "Reflection for cycle 1" in states_seen[1].reflection_history
    # Should have cycle 1's messages
    assert len(states_seen[1].message_history) == 1
```

### Manual Testing Checklist

Before marking story complete:
- [ ] Run experiment with cycle_count=3
- [ ] Check console output for cycle messages
- [ ] Open log file
- [ ] Verify CYCLE_END events have final_reflection in payload
- [ ] Verify each reflection is different (shows state continuity)
- [ ] Verify cycle 2+ reflections reference previous cycles
- [ ] Run experiment twice with same run_id (should append to log)
- [ ] Verify both runs' data logged correctly

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
