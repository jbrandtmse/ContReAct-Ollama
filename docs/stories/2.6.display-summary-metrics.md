# Story 2.6: Display Summary Metrics on Dashboard

**Status**: Ready for Review

---

## Story

**As a** User,
**I want** to see the key summary metrics of a selected run at a glance,
**so that** I can quickly understand the high-level results.

---

## Acceptance Criteria

1. After a run is selected, the dashboard displays key summary metrics (e.g., total memory operations, messages to operator) using `st.metric` widgets
2. A complete table of all summary metrics for the run is displayed using `st.dataframe`
3. If a PEI assessment log exists for the run, its results are also displayed in a table

---

## Tasks / Subtasks

- [x] **Task 1: Extract Summary Metrics from DataFrame** (AC: 1, 2)
  - [x] Filter DataFrame for CYCLE_END events
  - [x] Extract metrics from payload field
  - [x] Aggregate metrics across all cycles
  - [x] Calculate summary statistics

- [x] **Task 2: Display Key Metrics with st.metric** (AC: 1)
  - [x] Create metrics layout with st.columns
  - [x] Display total memory operations
  - [x] Display total messages to operator
  - [x] Display total response characters
  - [x] Display total memory write characters

- [x] **Task 3: Display Full Metrics Table** (AC: 2)
  - [x] Create DataFrame of all metrics
  - [x] Format for display
  - [x] Show using st.dataframe
  - [x] Enable sorting/filtering

- [x] **Task 4: Check for PEI Assessment** (AC: 3)
  - [x] Look for PEI assessment file
  - [x] Load PEI results if exists
  - [x] Parse PEI data
  - [x] Display in table format

- [x] **Task 5: Error Handling** (AC: 1, 2, 3)
  - [x] Handle missing metrics gracefully
  - [x] Handle malformed payload data
  - [x] Show informative messages
  - [x] Prevent crashes

- [x] **Task 6: Testing** (AC: 1, 2, 3)
  - [x] Test metrics display correctly
  - [x] Test full table shows all data
  - [x] Test PEI assessment display
  - [x] Test missing data handling

---

## Dev Notes

### Previous Story Insights
From Story 2.5:
- DataFrame loaded in session_state.run_data
- Current run stored in session_state.current_run
- Data includes timestamp, run_id, cycle_number, event_type, payload

From data-models.md:
- CYCLE_END events contain metrics in payload
- Metrics include: memory_ops_total, messages_to_operator, response_chars, memory_write_chars

### Implementation Details

**Add to pages/2_ðŸ“Š_Results_Dashboard.py** (after run selector section):

```python
import streamlit as st
import pandas as pd
import json
from pathlib import Path

# ... existing imports and functions ...

# After run loading section, add metrics display
if 'run_data' in st.session_state:
    df = st.session_state.run_data
    
    st.divider()
    st.subheader("ðŸ“ˆ Summary Metrics")
    
    # Extract CYCLE_END events
    cycle_ends = df[df['event_type'] == 'CYCLE_END'].copy()
    
    if len(cycle_ends) > 0:
        # Extract metrics from payload
        metrics_list = []
        for idx, row in cycle_ends.iterrows():
            payload = row['payload']
            if 'metrics' in payload:
                metrics = payload['metrics']
                metrics['cycle_number'] = row['cycle_number']
                metrics_list.append(metrics)
        
        if metrics_list:
            metrics_df = pd.DataFrame(metrics_list)
            
            # Calculate totals
            total_memory_ops = metrics_df['memory_ops_total'].sum()
            total_messages = metrics_df['messages_to_operator'].sum()
            total_response_chars = metrics_df['response_chars'].sum()
            total_memory_chars = metrics_df['memory_write_chars'].sum()
            
            # Display key metrics with st.metric
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    label="Total Memory Operations",
                    value=total_memory_ops
                )
            
            with col2:
                st.metric(
                    label="Messages to Operator",
                    value=total_messages
                )
            
            with col3:
                st.metric(
                    label="Response Characters",
                    value=f"{total_response_chars:,}"
                )
            
            with col4:
                st.metric(
                    label="Memory Write Characters",
                    value=f"{total_memory_chars:,}"
                )
            
            st.divider()
            
            # Display full metrics table
            st.subheader("ðŸ“Š Detailed Metrics by Cycle")
            
            # Reorder columns for better display
            display_columns = ['cycle_number', 'memory_ops_total', 'messages_to_operator', 
                             'response_chars', 'memory_write_chars']
            metrics_df = metrics_df[display_columns]
            
            # Rename for clarity
            metrics_df.columns = ['Cycle', 'Memory Ops', 'Messages', 
                                  'Response Chars', 'Memory Chars']
            
            st.dataframe(
                metrics_df,
                use_container_width=True,
                hide_index=True
            )
        else:
            st.warning("No metrics found in CYCLE_END events")
    else:
        st.warning("No CYCLE_END events found in log file")
    
    st.divider()
    
    # Check for PEI assessment
    st.subheader("ðŸ§  PEI Assessment Results")
    
    pei_file = Path(f"logs/{st.session_state.current_run}_pei.json")
    
    if pei_file.exists():
        try:
            with open(pei_file, 'r', encoding='utf-8') as f:
                pei_data = json.load(f)
            
            st.success("âœ… PEI Assessment found")
            
            # Display PEI results
            if isinstance(pei_data, dict):
                # Convert to DataFrame for display
                pei_df = pd.DataFrame([pei_data])
                st.dataframe(pei_df, use_container_width=True)
            elif isinstance(pei_data, list):
                pei_df = pd.DataFrame(pei_data)
                st.dataframe(pei_df, use_container_width=True)
            else:
                st.json(pei_data)
                
        except json.JSONDecodeError as e:
            st.error(f"Error parsing PEI assessment file: {e}")
        except Exception as e:
            st.error(f"Error loading PEI assessment: {e}")
    else:
        st.info(f"""
        No PEI assessment found for this run.
        
        **To run PEI assessment:**
        ```
        python run_pei_assessment.py \\
          --log logs/{st.session_state.current_run}.jsonl \\
          --evaluator llama3:latest \\
          --output logs/{st.session_state.current_run}_pei.json
        ```
        """)
```

### Metrics Calculation

**Metrics from CYCLE_END Payload**:
- `memory_ops_total`: Total memory operations in cycle
- `messages_to_operator`: Messages sent to operator in cycle
- `response_chars`: Character count of LLM responses
- `memory_write_chars`: Character count written to memory

**Aggregation Strategy**:
- Sum across all cycles for totals
- Display per-cycle breakdown in table
- Use formatting for large numbers (commas)

### PEI Assessment File Format

**Expected File**: `logs/{run_id}_pei.json`

**Expected Structure** (from Story 2.9):
```json
{
  "run_id": "llama3-exploration-A",
  "evaluator_model": "llama3:latest",
  "pei_rating": 6,
  "pei_response": "Full text response from evaluator...",
  "timestamp": "2025-01-08T15:30:00.000000"
}
```

### UI Layout

**Metrics Section**:
1. Four-column layout for key metrics
2. Full-width table for detailed metrics
3. Separate section for PEI results

**Error States**:
- No CYCLE_END events: Show warning
- No metrics in payload: Show informative message
- No PEI file: Show instructions to run assessment

### File Locations
- **Results Dashboard Page**: `pages/2_ðŸ“Š_Results_Dashboard.py`
- **Log Files**: `logs/*.jsonl`
- **PEI Files**: `logs/*_pei.json`

---

## Testing

### Testing Requirements for Story 2.6

**Manual Testing Checklist**:

1. **test_key_metrics_display**
   - Load a completed run
   - Verify 4 metric widgets appear
   - Verify totals calculated correctly
   - Verify number formatting (commas)

2. **test_metrics_table_display**
   - Load a run with multiple cycles
   - Verify table shows all cycles
   - Verify columns renamed properly
   - Verify table is sortable

3. **test_pei_assessment_found**
   - Create PEI assessment file for a run
   - Load that run in dashboard
   - Verify PEI section shows success message
   - Verify PEI data displayed in table

4. **test_pei_assessment_not_found**
   - Load a run without PEI assessment
   - Verify info message displays
   - Verify command example shown
   - Verify correct run_id in command

5. **test_missing_metrics**
   - Load log with CYCLE_END but no metrics
   - Verify warning message appears
   - Verify application doesn't crash

6. **test_no_cycle_end_events**
   - Load incomplete log file
   - Verify warning about no CYCLE_END events
   - Verify application handles gracefully

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (cline-agent)

### Debug Log References
None - Implementation completed without issues.

### Completion Notes List
- Added three new utility functions to `contreact_ollama/ui_utils.py` for testability:
  - `extract_metrics_from_dataframe()`: Extracts metrics from CYCLE_END events
  - `calculate_summary_metrics()`: Calculates totals from metrics DataFrame
  - `load_pei_assessment()`: Loads PEI assessment JSON files
- Implemented metrics display in Results Dashboard with 4-column layout showing key totals
- Added detailed metrics table with per-cycle breakdown
- Implemented PEI assessment loading and display with helpful instructions when not found
- All error handling integrated throughout implementation
- Comprehensive test coverage: 25 tests, all passing
- Tests cover metrics extraction, summary calculations, PEI loading, and edge cases

### File List
**Modified Files:**
- `pages/2_ðŸ“Š_results_dashboard.py` - Added summary metrics display, detailed metrics table, and PEI assessment section
- `contreact_ollama/ui_utils.py` - Added extract_metrics_from_dataframe, calculate_summary_metrics, load_pei_assessment functions
- `tests/unit/test_results_dashboard.py` - Added 14 new test methods for metrics extraction, summary calculations, and PEI loading

**No new files created.**

---

## QA Results
*To be populated by QA agent after implementation*
