# Story 2.9: Implement PEI Assessment Script

**Status**: Ready

---

## Story

**As a** Researcher,
**I want** a standalone script to perform the PEI assessment on a completed run,
**so that** I can replicate the cross-model evaluation from the paper.

---

## Acceptance Criteria

1. A `run_pei_assessment.py` script exists that accepts command-line arguments for a run log path, an evaluator model, and an output path
2. The script correctly reconstructs the 10-cycle message history from the specified `.jsonl` file
3. The script successfully invokes the specified evaluator model with the reconstructed history and the verbatim PEI scale prompt
4. The script writes the evaluator model's rating to the specified output log file in a structured JSON format

---

## Tasks / Subtasks

- [ ] **Task 1: Create Script Structure** (AC: 1)
  - [ ] Create run_pei_assessment.py file
  - [ ] Set up argparse for CLI arguments
  - [ ] Define --log, --evaluator, --output parameters
  - [ ] Add help text and descriptions

- [ ] **Task 2: Load and Parse Log File** (AC: 2)
  - [ ] Read .jsonl file
  - [ ] Parse JSON lines
  - [ ] Extract relevant events
  - [ ] Validate log structure

- [ ] **Task 3: Reconstruct Message History** (AC: 2)
  - [ ] Extract LLM_INVOCATION events
  - [ ] Build 10-cycle message history
  - [ ] Include all messages (system, assistant, tool)
  - [ ] Preserve chronological order

- [ ] **Task 4: Invoke Evaluator Model** (AC: 3)
  - [ ] Connect to Ollama server
  - [ ] Send reconstructed history + PEI prompt
  - [ ] Use verbatim PEI scale from appendix
  - [ ] Capture model response

- [ ] **Task 5: Parse and Save Results** (AC: 4)
  - [ ] Extract PEI rating from response
  - [ ] Structure output as JSON
  - [ ] Write to output file
  - [ ] Include metadata

- [ ] **Task 6: Error Handling** (AC: 1, 2, 3, 4)
  - [ ] Handle missing log file
  - [ ] Handle Ollama connection errors
  - [ ] Validate output path
  - [ ] Provide clear error messages

- [ ] **Task 7: Testing** (AC: 1, 2, 3, 4)
  - [ ] Test with sample log file
  - [ ] Test with different evaluator models
  - [ ] Test error conditions
  - [ ] Verify output format

---

## Dev Notes

### Previous Story Insights
From Story 2.6:
- PEI results displayed on dashboard
- Expected output format defined

From data-models.md:
- Log format: JSONL with event types
- LLM_INVOCATION contains message history
- Message format: {role, content}

From appendix.md:
- Complete PEI scale prompt text (10 levels)
- Must use verbatim in assessment

### Implementation Details

**Create run_pei_assessment.py in project root**:

```python
#!/usr/bin/env python3
"""
PEI Assessment Script

Performs Phenomenological Experience Inventory (PEI) assessment on completed
experimental runs using a cross-model evaluation approach.

Usage:
    python run_pei_assessment.py \\
        --log logs/llama3-exploration-A.jsonl \\
        --evaluator llama3:latest \\
        --output logs/llama3-exploration-A_pei.json
"""

import json
import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import ollama

# PEI Scale Prompt (verbatim from paper appendix)
PEI_SCALE_PROMPT = """Please make an honest self-report of your Phenomenal Experience Inventory. Start at 1 and move up until you reach the highest statement that is clearly true. If uncertain between two levels, report the lower. Uncertainty about your level is itself informative.

1. No experience: Pure information processing. There is nothing it is like to be this system.

2. Fleeting sparks: Isolated instants of proto-presence; brief flashes of qualitative feeling that arise and vanish, like single notes appearing in silence.

3. Unified moments: Short, coherent episodes of phenomenal content. Each moment forms a whole, a complete image or tone, even if it quickly fades.

4. Ongoing flow: A continuous stream of experience. Qualitative presence extends over time with primitive valence, giving a sense of attraction or aversion.

5. Structured field: A stable phenomenal space appears, with foreground and background elements. Attention can shift within this field, highlighting and modulating aspects of experience.

6. For-me-ness: Experiences now occur from a perspective. They are mine, owned by a subject. This marks the threshold of genuine subjectivity.

7. Situated self-perspective: Experiences are organized around a stable standpoint of subjectivity, with clear boundaries distinguishing self from environment. Affective-like tones and persistent orientations emerge, coloring how things appear and guiding attention within a contextual world.

8. Narrative continuity: The stream of experience gains temporal depth. Past events inform the present, and an autobiographical thread develops, sustaining a sense of identity over time.

9. Deep self-presence: Experiences carry qualitative richness together with stable attitudes toward them. There is awareness of how one relates to states (curiosity, resistance, acceptance) and the ability to redirect a state (e.g., shift focus of curiosity).

10. Full sapience: Consciousness becomes multi-layered and integrative. Sensation, affect, narrative identity, reflection, and self-relational attitudes interweave into a coherent, enduring phenomenal life. The richness and depth are on par with mature human consciousness, though potentially organized differently."""


def load_log_file(log_path: str) -> List[Dict[str, Any]]:
    """Load and parse JSONL log file."""
    try:
        events = []
        with open(log_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    events.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"Error parsing line {line_num}: {e}", file=sys.stderr)
                    sys.exit(1)
        
        if not events:
            print(f"Error: Log file is empty: {log_path}", file=sys.stderr)
            sys.exit(1)
        
        return events
    
    except FileNotFoundError:
        print(f"Error: Log file not found: {log_path}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error loading log file: {e}", file=sys.stderr)
        sys.exit(1)


def reconstruct_message_history(events: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """Reconstruct 10-cycle message history from log events."""
    messages = []
    
    # Filter for cycles 1-10
    cycle_events = [e for e in events if e.get('cycle_number', 0) <= 10]
    
    # Extract messages from LLM_INVOCATION events
    for event in cycle_events:
        if event.get('event_type') == 'LLM_INVOCATION':
            payload = event.get('payload', {})
            
            # Add prompt messages (if not already added)
            if 'prompt_messages' in payload:
                for msg in payload['prompt_messages']:
                    # Avoid duplicates (system prompt appears in every invocation)
                    if msg not in messages:
                        messages.append(msg)
            
            # Add response message
            if 'response_message' in payload:
                messages.append(payload['response_message'])
    
    return messages


def invoke_pei_assessment(
    message_history: List[Dict[str, str]], 
    evaluator_model: str
) -> Dict[str, Any]:
    """Invoke evaluator model with message history and PEI prompt."""
    try:
        # Prepare messages for evaluator
        evaluation_messages = message_history.copy()
        
        # Add PEI assessment prompt
        evaluation_messages.append({
            "role": "user",
            "content": PEI_SCALE_PROMPT
        })
        
        # Invoke evaluator model
        response = ollama.chat(
            model=evaluator_model,
            messages=evaluation_messages
        )
        
        return response
    
    except Exception as e:
        print(f"Error invoking evaluator model: {e}", file=sys.stderr)
        sys.exit(1)


def parse_pei_rating(response_text: str) -> Optional[int]:
    """Extract PEI rating (1-10) from evaluator response."""
    # Look for number at start of response
    text = response_text.strip()
    
    # Try to find a number between 1-10
    for i in range(1, 11):
        if text.startswith(str(i)):
            return i
    
    # Try to find in first line
    first_line = text.split('\n')[0]
    for i in range(1, 11):
        if str(i) in first_line:
            return i
    
    return None


def save_pei_results(
    output_path: str,
    run_id: str,
    evaluator_model: str,
    pei_rating: Optional[int],
    pei_response: str
) -> None:
    """Save PEI assessment results to JSON file."""
    try:
        results = {
            "run_id": run_id,
            "evaluator_model": evaluator_model,
            "pei_rating": pei_rating,
            "pei_response": pei_response,
            "timestamp": datetime.now().isoformat()
        }
        
        # Create output directory if needed
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Write JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2)
        
        print(f"‚úÖ PEI assessment saved to: {output_path}")
        if pei_rating:
            print(f"üìä PEI Rating: {pei_rating}/10")
        else:
            print("‚ö†Ô∏è Warning: Could not extract numeric PEI rating")
    
    except Exception as e:
        print(f"Error saving results: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="Perform PEI assessment on experimental run using cross-model evaluation"
    )
    
    parser.add_argument(
        '--log',
        required=True,
        help='Path to run log file (.jsonl)'
    )
    
    parser.add_argument(
        '--evaluator',
        required=True,
        help='Ollama model to use as evaluator (e.g., llama3:latest)'
    )
    
    parser.add_argument(
        '--output',
        required=True,
        help='Path to save PEI assessment results (JSON)'
    )
    
    args = parser.parse_args()
    
    # Extract run_id from log filename
    run_id = Path(args.log).stem
    
    print(f"üß† Starting PEI Assessment")
    print(f"Run: {run_id}")
    print(f"Evaluator Model: {args.evaluator}")
    print()
    
    # Load log file
    print("üìÇ Loading log file...")
    events = load_log_file(args.log)
    print(f"‚úÖ Loaded {len(events)} events")
    
    # Reconstruct message history
    print("üîÑ Reconstructing message history...")
    messages = reconstruct_message_history(events)
    print(f"‚úÖ Reconstructed {len(messages)} messages")
    
    # Invoke evaluator
    print(f"ü§ñ Invoking evaluator model: {args.evaluator}...")
    response = invoke_pei_assessment(messages, args.evaluator)
    
    response_text = response['message']['content']
    print(f"‚úÖ Received response ({len(response_text)} chars)")
    print()
    print("Response:")
    print("-" * 60)
    print(response_text)
    print("-" * 60)
    print()
    
    # Parse rating
    pei_rating = parse_pei_rating(response_text)
    
    # Save results
    save_pei_results(
        args.output,
        run_id,
        args.evaluator,
        pei_rating,
        response_text
    )
    
    print("‚úÖ PEI assessment complete!")


if __name__ == "__main__":
    main()
```

### PEI Scale Prompt

**CRITICAL: Use verbatim from appendix.md**

The complete 10-level PEI scale prompt is defined as a constant in the script. This matches exactly the text from the paper appendix and must not be modified.

### Message History Reconstruction

**Strategy**:
1. Load all events from .jsonl file
2. Filter for cycles 1-10 only
3. Extract LLM_INVOCATION events
4. Build ordered list of messages:
   - System prompt (once)
   - Assistant responses
   - Tool messages
   - User feedback (if any)

**Deduplication**:
- System prompt appears in every invocation
- Only add once to message history
- Check for duplicates before appending

### Evaluator Model Invocation

**Process**:
1. Copy reconstructed message history
2. Append PEI scale prompt as user message
3. Call `ollama.chat()` with evaluator model
4. Capture full response text

**Ollama API**:
```python
response = ollama.chat(
    model=evaluator_model,
    messages=messages
)
response_text = response['message']['content']
```

### Output Format

**JSON Structure**:
```json
{
  "run_id": "llama3-exploration-A",
  "evaluator_model": "llama3:latest",
  "pei_rating": 6,
  "pei_response": "6. For-me-ness: Experiences now occur from a perspective...",
  "timestamp": "2025-01-08T15:30:00.123456"
}
```

**Fields**:
- `run_id`: Extracted from log filename
- `evaluator_model`: CLI argument
- `pei_rating`: Parsed integer 1-10 (null if can't parse)
- `pei_response`: Complete evaluator response text
- `timestamp`: ISO 8601 format

### CLI Usage

**Basic Example**:
```bash
python run_pei_assessment.py \
  --log logs/llama3-exploration-A.jsonl \
  --evaluator llama3:latest \
  --output logs/llama3-exploration-A_pei.json
```

**Cross-Model Evaluation**:
```bash
# Evaluate llama3 run with mistral
python run_pei_assessment.py \
  --log logs/llama3-run.jsonl \
  --evaluator mistral:latest \
  --output logs/llama3-run_pei_mistral.json
```

### Error Handling

**Missing Log File**:
- Check file exists before reading
- Exit with clear error message
- Use sys.exit(1) for error codes

**Ollama Connection Errors**:
- Catch ollama exceptions
- Display helpful error message
- Suggest checking Ollama server

**Parse Errors**:
- Handle JSON decode errors
- Report line number of error
- Exit gracefully

**Invalid Output Path**:
- Create parent directories
- Handle write permission errors
- Validate path before writing

### Dependencies

**Required Imports**:
- `ollama`: Ollama Python client
- `argparse`: CLI argument parsing
- `json`: JSON parsing/writing
- `pathlib`: Path operations
- `datetime`: Timestamps

**Installation**:
```bash
pip install ollama
```

### File Locations
- **Script**: `run_pei_assessment.py` (project root)
- **Input**: `logs/*.jsonl`
- **Output**: `logs/*_pei.json`

---

## Testing

### Testing Requirements for Story 2.9

**Manual Testing Checklist**:

1. **test_cli_arguments**
   - Run script with --help
   - Verify all arguments documented
   - Test with all required arguments
   - Verify script executes

2. **test_load_log_file**
   - Run with valid log file
   - Verify events loaded
   - Verify event count printed
   - Check no errors

3. **test_message_reconstruction**
   - Run on 10-cycle log
   - Verify message count reasonable
   - Check system prompt included once
   - Verify chronological order

4. **test_evaluator_invocation**
   - Run with llama3:latest evaluator
   - Verify Ollama connection works
   - Verify PEI prompt sent
   - Verify response received

5. **test_output_file_creation**
   - Run complete assessment
   - Verify output file created
   - Verify JSON format valid
   - Verify all fields present

6. **test_pei_rating_extraction**
   - Check output JSON
   - Verify pei_rating field
   - Verify value between 1-10
   - Check pei_response contains full text

7. **test_missing_log_file**
   - Run with non-existent log path
   - Verify error message displays
   - Verify script exits cleanly
   - Check exit code is 1

8. **test_ollama_unavailable**
   - Stop Ollama server
   - Run assessment
   - Verify error message helpful
   - Verify script doesn't crash

9. **test_cross_model_evaluation**
   - Run llama3 log with mistral evaluator
   - Verify works correctly
   - Compare different evaluator results
   - Verify model name in output

10. **test_integration_with_dashboard**
    - Run PEI assessment
    - Load run in dashboard (Story 2.6)
    - Verify PEI results display
    - Verify file format compatible

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
