# Story 2.7: Display Raw Conversation Log on Dashboard

**Status**: Ready

---

## Story

**As a** User,
**I want** to be able to view the detailed conversation history of a run,
**so that** I can perform a deep analysis of the agent's reasoning.

---

## Acceptance Criteria

1. The dashboard includes an expandable section implemented with `st.expander`
2. When expanded, this section displays the raw conversation history (e.g., thoughts, tool calls, reflections) from the loaded log file
3. The display is formatted for readability

---

## Tasks / Subtasks

- [ ] **Task 1: Create Expandable Section** (AC: 1)
  - [ ] Add st.expander widget to dashboard
  - [ ] Position after metrics section
  - [ ] Label appropriately
  - [ ] Configure default collapsed state

- [ ] **Task 2: Extract Conversation Data** (AC: 2)
  - [ ] Filter for LLM_INVOCATION events
  - [ ] Extract message history from payload
  - [ ] Extract tool calls from TOOL_CALL events
  - [ ] Extract reflections from CYCLE_END events

- [ ] **Task 3: Format for Display** (AC: 3)
  - [ ] Create readable format for messages
  - [ ] Distinguish message roles visually
  - [ ] Format tool calls clearly
  - [ ] Highlight reflections

- [ ] **Task 4: Handle Different Event Types** (AC: 2)
  - [ ] Process system messages
  - [ ] Process user messages
  - [ ] Process assistant messages
  - [ ] Process tool responses

- [ ] **Task 5: Error Handling** (AC: 2, 3)
  - [ ] Handle missing payload data
  - [ ] Handle malformed messages
  - [ ] Show informative messages
  - [ ] Prevent display errors

- [ ] **Task 6: Testing** (AC: 1, 2, 3)
  - [ ] Test expander opens/closes
  - [ ] Test all message types display
  - [ ] Test formatting is readable
  - [ ] Test error handling

---

## Dev Notes

### Previous Story Insights
From Story 2.5:
- DataFrame loaded in session_state.run_data
- Events include LLM_INVOCATION, TOOL_CALL, CYCLE_END

From Story 2.6:
- Metrics section already displayed
- Data extraction patterns established

From data-models.md:
- LLM_INVOCATION payload contains prompt_messages and response_message
- TOOL_CALL payload contains tool_name, parameters, output
- Message format: {"role": "...", "content": "..."}

### Implementation Details

**Add to pages/2_üìä_Results_Dashboard.py** (after metrics section):

```python
import streamlit as st
import pandas as pd
import json

# ... existing code ...

# After metrics and PEI sections, add conversation log
if 'run_data' in st.session_state:
    df = st.session_state.run_data
    
    st.divider()
    
    with st.expander("üí¨ Raw Conversation Log", expanded=False):
        st.markdown("### Full Conversation History")
        st.caption("Complete message history and tool interactions from the experimental run")
        
        # Process events chronologically
        for idx, row in df.iterrows():
            cycle = row['cycle_number']
            event_type = row['event_type']
            payload = row['payload']
            timestamp = row['timestamp']
            
            if event_type == 'CYCLE_START':
                st.markdown(f"---")
                st.markdown(f"### üîÑ **Cycle {cycle} Start**")
                st.caption(f"‚è∞ {timestamp}")
            
            elif event_type == 'LLM_INVOCATION':
                st.markdown(f"#### ü§ñ LLM Invocation (Cycle {cycle})")
                
                # Display prompt messages
                if 'prompt_messages' in payload:
                    st.markdown("**Prompt Messages:**")
                    for msg in payload['prompt_messages']:
                        role = msg.get('role', 'unknown')
                        content = msg.get('content', '')
                        
                        if role == 'system':
                            st.info(f"**[SYSTEM]**\n\n{content}")
                        elif role == 'user':
                            st.success(f"**[USER]**\n\n{content}")
                        elif role == 'assistant':
                            st.warning(f"**[ASSISTANT]**\n\n{content}")
                        elif role == 'tool':
                            st.error(f"**[TOOL]**\n\n{content}")
                
                # Display response
                if 'response_message' in payload:
                    st.markdown("**Response:**")
                    response = payload['response_message']
                    role = response.get('role', 'assistant')
                    content = response.get('content', '')
                    st.warning(f"**[{role.upper()}]**\n\n{content}")
            
            elif event_type == 'TOOL_CALL':
                st.markdown(f"#### üîß Tool Call (Cycle {cycle})")
                
                tool_name = payload.get('tool_name', 'unknown')
                parameters = payload.get('parameters', {})
                output = payload.get('output', '')
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    st.markdown(f"**Tool:** `{tool_name}`")
                    st.json(parameters)
                
                with col2:
                    st.markdown(f"**Output:**")
                    st.code(output, language=None)
            
            elif event_type == 'CYCLE_END':
                st.markdown(f"#### üí≠ Final Reflection (Cycle {cycle})")
                
                if 'final_reflection' in payload:
                    reflection = payload['final_reflection']
                    st.info(f"**Reflection:**\n\n{reflection}")
                
                if 'metrics' in payload:
                    metrics = payload['metrics']
                    st.caption(f"üìä Memory Ops: {metrics.get('memory_ops_total', 0)} | "
                             f"Messages: {metrics.get('messages_to_operator', 0)} | "
                             f"Response Chars: {metrics.get('response_chars', 0)}")
        
        st.markdown("---")
        st.success("‚úÖ End of conversation log")
```

### Conversation Log Structure

**Event Processing Order**:
1. CYCLE_START - Mark beginning of cycle
2. LLM_INVOCATION - Show prompts and responses
3. TOOL_CALL - Show tool interactions (may be multiple)
4. CYCLE_END - Show reflection and metrics

**Message Role Formatting**:
- System: Blue info box
- User: Green success box
- Assistant: Yellow warning box
- Tool: Red error box

### Visual Formatting

**Layout Strategy**:
- Use markdown headers for structure
- Use st.info/success/warning/error for role distinction
- Use st.code for tool outputs
- Use st.json for parameters
- Use st.caption for timestamps and metadata

**Readability Features**:
- Horizontal dividers between cycles
- Collapsed by default (expander)
- Clear role labels in brackets
- Timestamp display for context

### Performance Considerations

**Large Log Files**:
- Expander prevents rendering until opened
- Render all events (no pagination needed for typical 10-cycle runs)
- For future: Consider pagination if logs exceed 100 cycles

**Memory Usage**:
- Data already in session_state
- No additional copies created
- Streamlit handles rendering efficiently

### File Locations
- **Results Dashboard Page**: `pages/2_üìä_Results_Dashboard.py`
- **Log Files**: `logs/*.jsonl`

---

## Testing

### Testing Requirements for Story 2.7

**Manual Testing Checklist**:

1. **test_expander_functionality**
   - Load a run in dashboard
   - Verify expander widget appears
   - Click to expand
   - Verify conversation displays
   - Click to collapse
   - Verify content hides

2. **test_cycle_structure_display**
   - Expand conversation log
   - Verify CYCLE_START markers appear
   - Verify cycle numbers correct
   - Verify chronological order maintained

3. **test_llm_invocation_display**
   - Verify prompt messages shown
   - Verify response message shown
   - Verify roles colored correctly
   - Verify message content readable

4. **test_tool_call_display**
   - Verify tool name displayed
   - Verify parameters shown as JSON
   - Verify output shown in code block
   - Verify two-column layout works

5. **test_reflection_display**
   - Verify final reflection shown at cycle end
   - Verify reflection text readable
   - Verify metrics summary displayed
   - Verify formatting consistent

6. **test_message_role_formatting**
   - Verify SYSTEM messages use blue info box
   - Verify USER messages use green success box
   - Verify ASSISTANT messages use yellow warning box
   - Verify TOOL messages use red error box

7. **test_full_run_display**
   - Load complete 10-cycle run
   - Expand conversation log
   - Verify all 10 cycles display
   - Verify no missing events
   - Verify end marker appears

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
