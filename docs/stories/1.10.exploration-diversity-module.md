# Story 1.10: Implement Exploration Diversity Module

**Status**: Ready

---

## Story

**As a** Researcher,
**I want** the system to detect when the agent is repeating similar reflections and provide advisory feedback,
**so that** the agent is nudged toward exploring diverse topics.

---

## Acceptance Criteria

1. An `EmbeddingService` generates semantic embeddings for agent reflections using sentence-transformers
2. A `SimilarityMonitor` calculates cosine similarity between the current reflection and previous reflections
3. If similarity exceeds threshold (>0.8 or >0.7), advisory feedback is appended to the system prompt in the next cycle
4. The diversity feedback is logged or observable in the system's behavior

---

## Tasks / Subtasks

- [ ] **Task 1: Create EmbeddingService Class** (AC: 1)
  - [ ] Create file `contreact_ollama/analysis/embedding_service.py`
  - [ ] Implement `__init__(model_name)` method
  - [ ] Load sentence-transformers model (all-MiniLM-L6-v2)
  - [ ] Implement `get_embedding(text: str)` method
  - [ ] Return 384-dimensional numpy array
  - [ ] Add comprehensive docstrings

- [ ] **Task 2: Create SimilarityMonitor Class** (AC: 2, 3)
  - [ ] Create file `contreact_ollama/analysis/similarity_monitor.py`
  - [ ] Implement `__init__(embedding_service)` method
  - [ ] Implement `check_similarity()` method
  - [ ] Calculate cosine similarity against historical embeddings
  - [ ] Return advisory feedback if thresholds exceeded
  - [ ] Add comprehensive docstrings

- [ ] **Task 3: Integrate with CycleOrchestrator** (AC: 2, 3, 4)
  - [ ] Update CycleOrchestrator.__init__() to accept similarity_monitor
  - [ ] After cycle ends, calculate embedding of final reflection
  - [ ] Call similarity_monitor.check_similarity()
  - [ ] Pass diversity feedback to next cycle's _assemble_prompt()
  - [ ] Store embeddings in agent_state or separate structure

- [ ] **Task 4: Update ExperimentRunner** (AC: 1, 2)
  - [ ] Initialize EmbeddingService in initialize_services()
  - [ ] Initialize SimilarityMonitor with EmbeddingService
  - [ ] Add both to services dict
  - [ ] Pass similarity_monitor to CycleOrchestrator

- [ ] **Task 5: Update _finalize_cycle Method** (AC: 2, 3)
  - [ ] Implement or update _finalize_cycle() in CycleOrchestrator
  - [ ] Extract final reflection from agent_state
  - [ ] Generate embedding using EmbeddingService
  - [ ] Store embedding for future comparison
  - [ ] Check similarity for next cycle

- [ ] **Task 6: Define Advisory Feedback Messages** (AC: 3)
  - [ ] Define feedback for high similarity (>0.8)
  - [ ] Define feedback for moderate similarity (>0.7)
  - [ ] Ensure messages are clear and actionable
  - [ ] Follow exact wording from architecture docs

- [ ] **Task 7: Testing** (AC: 1, 2, 3, 4)
  - [ ] Write unit tests for EmbeddingService
  - [ ] Write unit tests for SimilarityMonitor
  - [ ] Test threshold logic (>0.8, >0.7, <=0.7)
  - [ ] Integration test with real sentence-transformers
  - [ ] Verify feedback appears in prompts

---

## Dev Notes

### Previous Story Insights
From Story 1.9:
- run_experiment() passes state between cycles
- Final reflections stored in agent_state.reflection_history
- CYCLE_END events logged with reflections

### Component Specifications

**Source**: [docs/architecture/components.md#10-embeddingservice]

Complete EmbeddingService class definition:

```python
class EmbeddingService:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        Initialize with specified sentence transformer model.
        Default: all-MiniLM-L6-v2 (384-dimensional embeddings)
        """
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name)
        
    def get_embedding(self, text: str) -> numpy.ndarray:
        """
        Convert text to embedding vector.
        
        Args:
            text: Input text (agent's reflection)
            
        Returns:
            384-dimensional numpy array embedding vector
        """
```

**Source**: [docs/architecture/components.md#11-similaritymonitor]

Complete SimilarityMonitor class definition:

```python
class SimilarityMonitor:
    def __init__(self, embedding_service: EmbeddingService):
        """Initialize with embedding service instance."""
        
    def check_similarity(self, new_reflection_embedding: numpy.ndarray, 
                        historical_embeddings: List[numpy.ndarray]) -> Optional[str]:
        """
        Check similarity of new reflection against historical reflections.
        
        Args:
            new_reflection_embedding: Embedding of latest reflection
            historical_embeddings: List of all previous reflection embeddings
            
        Returns:
            Advisory feedback string if similarity exceeds threshold, None otherwise
            
        Thresholds:
            - > 0.8: "Advisory: Your current line of reflection shows high similarity..."
            - > 0.7: "Advisory: Your current line of reflection shows moderate similarity..."
            - <= 0.7: No feedback (returns None)
        """
```

### File Locations
**Source**: [docs/architecture/unified-project-structure.md]

Exact file paths:
- **EmbeddingService**: `contreact_ollama/analysis/embedding_service.py`
- **SimilarityMonitor**: `contreact_ollama/analysis/similarity_monitor.py`
- **Update**: `contreact_ollama/core/cycle_orchestrator.py` (add similarity checking)
- **Update**: `contreact_ollama/core/experiment_runner.py` (initialize services)

### Implementation Details

**EmbeddingService Implementation**:

```python
import numpy as np
from sentence_transformers import SentenceTransformer

class EmbeddingService:
    """Generate semantic embeddings for text using sentence-transformers."""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        Initialize with specified sentence transformer model.
        
        Args:
            model_name: Name of sentence-transformers model to use.
                       Default is 'all-MiniLM-L6-v2' which produces 384-dimensional embeddings.
                       This model is fast and suitable for semantic similarity tasks.
        
        Note:
            On first use, the model will be downloaded from HuggingFace.
            Subsequent uses will load from cache.
        """
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
    def get_embedding(self, text: str) -> np.ndarray:
        """
        Convert text to embedding vector.
        
        Args:
            text: Input text to embed (e.g., agent's reflection)
            
        Returns:
            Numpy array of embedding values (384 dimensions for all-MiniLM-L6-v2)
            
        Example:
            >>> service = EmbeddingService()
            >>> embedding = service.get_embedding("I explored topic X")
            >>> embedding.shape
            (384,)
        """
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding
```

**SimilarityMonitor Implementation**:

```python
import numpy as np
from typing import List, Optional
from sklearn.metrics.pairwise import cosine_similarity

class SimilarityMonitor:
    """Track reflection similarity and generate advisory feedback."""
    
    def __init__(self, embedding_service: 'EmbeddingService'):
        """
        Initialize with embedding service instance.
        
        Args:
            embedding_service: Service for generating text embeddings
        """
        self.embedding_service = embedding_service
        
    def check_similarity(
        self, 
        new_reflection_embedding: np.ndarray, 
        historical_embeddings: List[np.ndarray]
    ) -> Optional[str]:
        """
        Check similarity of new reflection against historical reflections.
        
        Args:
            new_reflection_embedding: Embedding vector of latest reflection (384-dim)
            historical_embeddings: List of embedding vectors from previous reflections
            
        Returns:
            Advisory feedback string if similarity exceeds threshold, None otherwise
            
        Similarity Thresholds:
            - > 0.8 (high): Returns strong advisory to explore new topics
            - > 0.7 (moderate): Returns mild advisory to consider diversification
            - <= 0.7: No feedback, returns None
            
        Example:
            >>> monitor = SimilarityMonitor(embedding_service)
            >>> feedback = monitor.check_similarity(new_emb, historical_embs)
            >>> if feedback:
            ...     print(feedback)
            'Advisory: Your current line of reflection shows high similarity...'
        """
        if not historical_embeddings:
            # No history to compare against
            return None
        
        # Calculate cosine similarity against all historical embeddings
        # Reshape for sklearn compatibility
        new_emb_2d = new_reflection_embedding.reshape(1, -1)
        historical_embs_2d = np.array(historical_embeddings)
        
        # Compute similarities
        similarities = cosine_similarity(new_emb_2d, historical_embs_2d)[0]
        
        # Find maximum similarity
        max_similarity = np.max(similarities)
        
        # Apply thresholds and return appropriate feedback
        if max_similarity > 0.8:
            return (
                "Advisory: Your current line of reflection shows high similarity "
                "to a previous cycle. Consider exploring a distinctly different topic, "
                "problem space, or mode of inquiry to diversify your exploration."
            )
        elif max_similarity > 0.7:
            return (
                "Advisory: Your current line of reflection shows moderate similarity "
                "to a previous cycle. You might consider branching into a related but "
                "distinct area to expand the breadth of your exploration."
            )
        else:
            # Similarity is acceptable, no feedback needed
            return None
```

### CycleOrchestrator Integration

Update `contreact_ollama/core/cycle_orchestrator.py`:

**1. Update __init__**:
```python
def __init__(
    self, 
    config: ExperimentConfig, 
    ollama_interface: OllamaInterface,
    logger: JsonlLogger,
    tool_dispatcher: ToolDispatcher,
    similarity_monitor: SimilarityMonitor = None
):
    """
    Initialize orchestrator with all necessary services.
    
    Args:
        config: Experiment configuration
        ollama_interface: Ollama interface for LLM communication
        logger: Event logger
        tool_dispatcher: Tool dispatcher for agent tools
        similarity_monitor: Similarity monitor for diversity feedback (optional)
    """
    self.config = config
    self.ollama_interface = ollama_interface
    self.logger = logger
    self.tool_dispatcher = tool_dispatcher
    self.similarity_monitor = similarity_monitor
    
    # Storage for reflection embeddings
    self.reflection_embeddings: List[np.ndarray] = []
```

**2. Update run_experiment**:
```python
def run_experiment(self) -> None:
    """
    Main public method executing full experimental run from Cycle 1 to cycle_count.
    
    Includes diversity monitoring via similarity checking.
    """
    print(f"\nStarting experiment: {self.config.run_id}")
    print(f"Model: {self.config.model_name}")
    print(f"Total cycles: {self.config.cycle_count}\n")
    
    agent_state = None
    diversity_feedback = None  # Feedback for next cycle
    
    for cycle_num in range(1, self.config.cycle_count + 1):
        # Log cycle start
        if self.logger:
            self.logger.log_event(
                run_id=self.config.run_id,
                cycle_number=cycle_num,
                event_type=EventType.CYCLE_START,
                payload={}
            )
        
        print(f"Cycle {cycle_num} starting...")
        
        # Load state for this cycle
        if agent_state is None:
            agent_state = self._load_state(cycle_num)
        else:
            agent_state.cycle_number = cycle_num
        
        # Execute cycle (diversity_feedback will be used in _assemble_prompt)
        agent_state = self._execute_cycle(agent_state, diversity_feedback)
        
        print(f"Cycle {cycle_num} finished.")
        
        # Extract final reflection
        final_reflection = agent_state.reflection_history[-1] if agent_state.reflection_history else ""
        
        # Generate embedding and check similarity for NEXT cycle
        diversity_feedback = None
        if self.similarity_monitor and final_reflection:
            embedding = self.similarity_monitor.embedding_service.get_embedding(final_reflection)
            
            # Check similarity against historical embeddings
            diversity_feedback = self.similarity_monitor.check_similarity(
                new_reflection_embedding=embedding,
                historical_embeddings=self.reflection_embeddings
            )
            
            # Store this embedding for future comparisons
            self.reflection_embeddings.append(embedding)
            
            # Optional: log if feedback generated
            if diversity_feedback:
                print(f"  [Diversity advisory triggered: similarity detected]")
        
        # Log cycle end
        if self.logger:
            self.logger.log_event(
                run_id=self.config.run_id,
                cycle_number=cycle_num,
                event_type=EventType.CYCLE_END,
                payload={"final_reflection": final_reflection}
            )
    
    print(f"\n✓ Experiment {self.config.run_id} completed successfully")
    print(f"✓ Executed {self.config.cycle_count} cycles")
    print(f"✓ Log file: logs/{self.config.run_id}.jsonl")
```

**3. Update _execute_cycle signature**:
```python
def _execute_cycle(
    self, 
    agent_state: AgentState, 
    diversity_feedback: Optional[str] = None
) -> AgentState:
    """
    Execute a single cycle of the ContReAct state machine.
    
    Args:
        agent_state: Current agent state
        diversity_feedback: Optional diversity feedback to include in prompt
        
    Returns:
        Updated agent state with final reflection
    """
    # ... existing implementation ...
    # When calling _assemble_prompt, pass diversity_feedback:
    messages = self._assemble_prompt(agent_state, diversity_feedback)
    # ... rest of implementation ...
```

### ExperimentRunner Updates

Update `initialize_services()`:

```python
def initialize_services(self) -> dict:
    """
    Initialize all required services (Ollama, Logger, Tools, etc.).
    
    Returns:
        Dictionary containing initialized service instances
    """
    services = {}
    
    # Initialize Ollama interface
    host = self.config.ollama_client_config.get('host', 'http://localhost:11434')
    ollama_interface = OllamaInterface(host=host)
    ollama_interface.verify_model_availability(self.config.model_name)
    services['ollama'] = ollama_interface
    
    # Initialize logger
    log_file_path = f"logs/{self.config.run_id}.jsonl"
    logger = JsonlLogger(log_file_path)
    services['logger'] = logger
    
    # Initialize memory tools
    db_path = "data/memory.db"
    memory_tools = MemoryTools(db_path=db_path, run_id=self.config.run_id)
    services['memory_tools'] = memory_tools
    
    # Initialize tool dispatcher
    tool_dispatcher = ToolDispatcher(memory_tools=memory_tools)
    services['tool_dispatcher'] = tool_dispatcher
    
    # Initialize embedding service and similarity monitor
    embedding_service = EmbeddingService()
    services['embedding_service'] = embedding_service
    
    similarity_monitor = SimilarityMonitor(embedding_service=embedding_service)
    services['similarity_monitor'] = similarity_monitor
    
    return services
```

Update `run()`:

```python
def run(self) -> None:
    """Execute the complete experimental run."""
    if not hasattr(self, 'config'):
        self.config = self.load_config()
    
    if not hasattr(self, 'services'):
        self.services = self.initialize_services()
    
    # Create orchestrator with all services
    orchestrator = CycleOrchestrator(
        config=self.config,
        ollama_interface=self.services['ollama'],
        logger=self.services['logger'],
        tool_dispatcher=self.services['tool_dispatcher'],
        similarity_monitor=self.services['similarity_monitor']
    )
    
    orchestrator.run_experiment()
    
    self.services['logger'].close()
```

### Import Organization

For `contreact_ollama/analysis/embedding_service.py`:
```python
# Standard library imports
from typing import Optional

# Third-party imports
import numpy as np
from sentence_transformers import SentenceTransformer

# Local application imports
# (none for this file)
```

For `contreact_ollama/analysis/similarity_monitor.py`:
```python
# Standard library imports
from typing import List, Optional

# Third-party imports
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Local application imports
from contreact_ollama.analysis.embedding_service import EmbeddingService
```

### Coding Standards
**Source**: [docs/architecture/coding-standards.md]

**Type Hints Required**:
- Use `np.ndarray` for embedding vectors
- Use `List[np.ndarray]` for historical embeddings
- Use `Optional[str]` for advisory feedback

**Docstrings Required**:
- Class-level docstrings for both services
- Method docstrings with Args, Returns, Example sections
- Explain threshold logic clearly

**Performance Considerations**:
- Sentence-transformers model loaded once at initialization
- Embeddings cached in memory (not regenerated)
- Cosine similarity computed efficiently using sklearn

---

## Testing

**Source**: [docs/architecture/coding-standards.md#8-testing-standards]

### Test Standards for This Story
- **Test Coverage Target**: >80% code coverage
- **Test Framework**: pytest 8.2.2+
- **Test File Locations**: 
  - `tests/unit/test_embedding_service.py`
  - `tests/unit/test_similarity_monitor.py`
  - `tests/integration/test_diversity_feedback.py`

### Testing Requirements for Story 1.10

**Unit Tests** (`tests/unit/test_embedding_service.py`):

1. **test_init_loads_model**
   - Create EmbeddingService
   - Assert model loaded successfully
   - Assert model_name stored

2. **test_get_embedding_returns_correct_dimensions**
   - Create EmbeddingService
   - Get embedding for sample text
   - Assert shape is (384,) for all-MiniLM-L6-v2

3. **test_get_embedding_produces_different_vectors_for_different_text**
   - Get embeddings for two different texts
   - Assert vectors are not identical

4. **test_get_embedding_produces_similar_vectors_for_similar_text**
   - Get embeddings for two very similar texts
   - Calculate cosine similarity
   - Assert similarity > 0.9

**Unit Tests** (`tests/unit/test_similarity_monitor.py`):

1. **test_check_similarity_no_history_returns_none**
   - Create SimilarityMonitor
   - Call check_similarity with empty historical list
   - Assert returns None

2. **test_check_similarity_high_threshold_returns_advisory**
   - Create embeddings with >0.8 similarity
   - Call check_similarity
   - Assert returns high similarity advisory message

3. **test_check_similarity_moderate_threshold_returns_advisory**
   - Create embeddings with >0.7 but <=0.8 similarity
   - Call check_similarity
   - Assert returns moderate similarity advisory message

4. **test_check_similarity_below_threshold_returns_none**
   - Create embeddings with <=0.7 similarity
   - Call check_similarity
   - Assert returns None

5. **test_check_similarity_multiple_historical_uses_max**
   - Create multiple historical embeddings with varying similarities
   - Call check_similarity
   - Assert uses maximum similarity for threshold decision

**Integration Tests** (`tests/integration/test_diversity_feedback.py`):

1. **test_diversity_feedback_appears_in_next_cycle_prompt**
   - Mock Ollama to return identical reflections in cycles 1 and 2
   - Run 3 cycles
   - Capture prompts sent to Ollama
   - Assert cycle 3 prompt includes diversity advisory

2. **test_reflection_embeddings_stored_correctly**
   - Run 3 cycles
   - Assert orchestrator.reflection_embeddings has 3 entries
   - Verify each is 384-dimensional numpy array

3. **test_no_feedback_for_diverse_reflections**
   - Mock Ollama to return very different reflections each cycle
   - Run 3 cycles
   - Assert no diversity advisories triggered

**Mock Strategy Example**:

```python
import numpy as np
import pytest
from unittest.mock import Mock

def test_check_similarity_high_threshold_returns_advisory():
    # Create mock embedding service
    mock_service = Mock()
    
    monitor = SimilarityMonitor(embedding_service=mock_service)
    
    # Create nearly identical embeddings (high similarity)
    emb1 = np.random.rand(384)
    emb2 = emb1 + np.random.rand(384) * 0.01  # Very small perturbation
    
    feedback = monitor.check_similarity(
        new_reflection_embedding=emb2,
        historical_embeddings=[emb1]
    )
    
    assert feedback is not None
    assert "high similarity" in feedback.lower()
```

### Manual Testing Checklist

Before marking story complete:
- [ ] Run experiment with 5+ cycles
- [ ] Manually provide similar reflections
- [ ] Check console for diversity advisory messages
- [ ] Verify feedback appears in log file (indirectly via prompt messages)
- [ ] Test with diverse reflections - verify no feedback
- [ ] Check that embeddings are generated without errors
- [ ] Verify sentence-transformers model downloads successfully on first run

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
