# Story 2.5: Implement Results Dashboard Run Selector and Data Loading

**Status**: Ready for Review

---

## Story

**As a** User,
**I want** to select a completed experiment run in the results dashboard,
**so that** I can view its data.

---

## Acceptance Criteria

1. The "Results Dashboard" page features a dropdown menu that lists available runs by scanning the `logs/` directory for `.jsonl` files
2. Selecting a run from the dropdown triggers the application to read the corresponding `.jsonl` file
3. The data from the selected log file is successfully parsed and loaded into a Pandas DataFrame without errors
4. If no `.jsonl` files exist in the `logs/` directory, a helpful message instructs the user to run an experiment first
5. If a log file is corrupted or contains invalid JSON, an error message is displayed and the application doesn't crash
6. A loading indicator (e.g., `st.spinner`) is shown while the log file is being read and parsed

---

## Tasks / Subtasks

- [x] **Task 1: Implement Log File Scanner** (AC: 1)
  - [x] Create function to scan logs/ directory
  - [x] Return list of .jsonl files
  - [x] Handle directory not found case
  - [x] Sort files by timestamp/name

- [x] **Task 2: Add Run Selector Dropdown** (AC: 1)
  - [x] Add st.selectbox to Results Dashboard page
  - [x] Populate with scanned log files
  - [x] Extract run_id from filename for display
  - [x] Handle selection changes

- [x] **Task 3: Implement Log File Loading** (AC: 2, 3, 6)
  - [x] Create function to read .jsonl file
  - [x] Parse each line as JSON
  - [x] Convert to Pandas DataFrame
  - [x] Show loading spinner during parse

- [x] **Task 4: Handle Empty Logs Directory** (AC: 4)
  - [x] Check if logs/ directory exists
  - [x] Check if directory is empty
  - [x] Display helpful message to user
  - [x] Suggest running experiment first

- [x] **Task 5: Error Handling** (AC: 5)
  - [x] Catch JSON parsing errors
  - [x] Catch file read errors
  - [x] Display user-friendly error messages
  - [x] Prevent application crash

- [x] **Task 6: Testing** (AC: 1, 2, 3, 4, 5, 6)
  - [x] Test dropdown lists all runs
  - [x] Test file loading and parsing
  - [x] Test empty directory message
  - [x] Test corrupted file handling
  - [x] Test loading indicator displays

---

## Dev Notes

### Previous Story Insights
From Story 2.4:
- File scanning pattern established for configs
- Session state pattern for selected files
- Error handling for missing/corrupted files

From Story 2.1:
- Results Dashboard page exists at pages/2_ðŸ“Š_Results_Dashboard.py
- Basic page structure in place

### Implementation Details

**Create pages/2_ðŸ“Š_Results_Dashboard.py** (or update if exists):

```python
import streamlit as st
import pandas as pd
import json
from pathlib import Path
from typing import Optional

# Function to scan for log files
def get_log_files() -> list[str]:
    """Scan logs/ directory for .jsonl files."""
    logs_dir = Path("logs")
    if not logs_dir.exists():
        return []
    
    jsonl_files = list(logs_dir.glob("*.jsonl"))
    # Sort by modification time (newest first)
    jsonl_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return [f.name for f in jsonl_files]

# Function to load log file
def load_log_file(filename: str) -> Optional[pd.DataFrame]:
    """Load .jsonl log file into DataFrame."""
    try:
        logs = []
        with open(f"logs/{filename}", 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    logs.append(json.loads(line))
                except json.JSONDecodeError as e:
                    st.error(f"Error parsing line {line_num}: {e}")
                    return None
        
        if not logs:
            st.warning("Log file is empty")
            return None
            
        return pd.DataFrame(logs)
    
    except FileNotFoundError:
        st.error(f"Log file not found: {filename}")
        return None
    except Exception as e:
        st.error(f"Error loading log file: {e}")
        return None

st.title("ðŸ“Š Results Dashboard")

# Run selector
st.subheader("Select Experiment Run")

log_files = get_log_files()

if log_files:
    selected_log = st.selectbox(
        "Choose a run to analyze",
        options=log_files,
        format_func=lambda x: x.replace('.jsonl', ''),  # Display without extension
        key="selected_run"
    )
    
    # Load selected log with spinner
    if selected_log:
        with st.spinner(f"Loading {selected_log}..."):
            df = load_log_file(selected_log)
        
        if df is not None:
            st.success(f"âœ… Loaded {len(df)} events from `{selected_log}`")
            
            # Store in session state for other components
            st.session_state.run_data = df
            st.session_state.current_run = selected_log.replace('.jsonl', '')
            
            # Show basic info
            st.info(f"""
            **Run ID**: {st.session_state.current_run}  
            **Total Events**: {len(df)}  
            **Event Types**: {', '.join(df['event_type'].unique())}
            """)
        else:
            st.error("Failed to load log file. Please check the file format.")
            if 'run_data' in st.session_state:
                del st.session_state.run_data
else:
    st.info("ðŸ“­ No experiment logs found.")
    st.markdown("""
    **To get started:**
    1. Create a configuration in the **ðŸ§ª Experiment Configuration** page
    2. Run an experiment using the CLI: `python run_experiment.py --config configs/your-config.yaml`
    3. Return here to view results
    """)

st.divider()

# Placeholder for future components
if 'run_data' in st.session_state:
    st.subheader("Analysis Sections")
    st.info("ðŸ“Š Summary metrics, charts, and logs will appear here in future stories.")
```

### Log File Format Reference

From docs/architecture/data-models.md:

**JSONL Format** - Each line is a complete JSON object:
```json
{
  "timestamp": "2025-01-08T10:30:15.123456",
  "run_id": "llama3-exploration-A",
  "cycle_number": 1,
  "event_type": "CYCLE_START",
  "payload": {}
}
```

**Event Types**:
- `CYCLE_START`: Beginning of cycle
- `LLM_INVOCATION`: LLM call with prompt and response
- `TOOL_CALL`: Tool execution
- `CYCLE_END`: End of cycle with metrics

### DataFrame Structure

After loading, DataFrame will have columns:
- `timestamp` (str): ISO 8601 timestamp
- `run_id` (str): Experiment identifier
- `cycle_number` (int): Cycle number (1-10)
- `event_type` (str): Event type
- `payload` (dict): Event-specific data

### Session State Variables

**Key Session State**:
- `st.session_state.run_data`: Loaded DataFrame
- `st.session_state.current_run`: Selected run_id
- Clear when new run selected

### File Locations
- **Results Dashboard Page**: `pages/2_ðŸ“Š_Results_Dashboard.py`
- **Log Files**: `logs/*.jsonl`

---

## Testing

### Testing Requirements for Story 2.5

**Manual Testing Checklist**:

1. **test_dropdown_lists_runs**
   - Create 2-3 log files in logs/ directory manually
   - Navigate to Results Dashboard
   - Verify dropdown shows all .jsonl files
   - Verify files displayed without .jsonl extension

2. **test_load_valid_log**
   - Select a valid log file from dropdown
   - Verify loading spinner appears
   - Verify success message with event count
   - Verify basic info displays (run ID, event types)

3. **test_parse_to_dataframe**
   - Load a log file
   - Check session_state.run_data exists
   - Verify DataFrame has correct columns
   - Verify all rows parsed correctly

4. **test_empty_logs_directory**
   - Delete all files from logs/ directory
   - Navigate to Results Dashboard
   - Verify helpful message displays
   - Verify instructions to run experiment

5. **test_corrupted_log_file**
   - Create log file with invalid JSON on line 5
   - Select the corrupted file
   - Verify error message appears
   - Verify application doesn't crash
   - Verify specific line number shown in error

6. **test_loading_indicator**
   - Select a moderately large log file
   - Verify spinner displays during load
   - Verify spinner disappears after load

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (via Cline)

### Debug Log References
None - Implementation completed without issues

### Completion Notes List
- Created reusable ui_utils module for testable dashboard functions
- Implemented get_log_files() to scan and sort log files by modification time
- Implemented load_log_file() with comprehensive error handling
- Added session state management for selected run data
- All 11 unit tests passing
- Dashboard provides helpful guidance when no logs exist

### File List
**New Files:**
- `contreact_ollama/ui_utils.py` - Utility functions for UI components
- `tests/unit/test_results_dashboard.py` - Unit tests for dashboard functions (11 tests)

**Modified Files:**
- `pages/2_ðŸ“Š_results_dashboard.py` - Updated with run selector and data loading functionality

---

## QA Results

### Review Date: 2025-10-13

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Assessment: EXCELLENT**

The implementation demonstrates high-quality code with proper separation of concerns. Creating `contreact_ollama/ui_utils.py` as a dedicated utility module is an excellent architectural decision that promotes testability and reusability. The code is clean, well-documented, and follows best practices consistently.

**Strengths:**
- Clean separation between utility functions and UI code
- Comprehensive error handling with user-friendly messages
- Defensive programming (empty line skipping, file existence checks)
- Proper session state management with cleanup on errors
- Excellent test coverage with 11 passing tests

### Refactoring Performed

No refactoring was required. The code is already well-structured and follows project standards.

### Compliance Check

- Coding Standards: âœ“ Full compliance
  - Type hints present on all functions
  - Google-style docstrings for all public functions
  - Proper naming conventions (snake_case)
  - Specific exception handling (no bare except)
  - Import organization follows standards
- Project Structure: âœ“ Compliant
  - Test structure mirrors source code
  - Module placement appropriate (ui_utils in contreact_ollama/)
- Testing Strategy: âœ“ Excellent
  - 11 comprehensive unit tests
  - Tests cover success paths, error paths, and edge cases
  - All tests passing
  - Test naming follows convention
- All ACs Met: âœ“ Verified (see detailed mapping below)

### Requirements Traceability

**AC1: Dropdown lists available runs**
- **Given** user navigates to Results Dashboard
- **When** logs/ directory contains .jsonl files
- **Then** dropdown displays all files sorted by modification time (newest first)
- **Tests:** `test_returns_all_jsonl_files`, `test_sorts_by_modification_time_newest_first`

**AC2: Selecting run triggers file reading**
- **Given** user selects a run from dropdown
- **When** selection is made
- **Then** corresponding .jsonl file is read
- **Tests:** `test_loads_valid_jsonl_file`

**AC3: Data parsed and loaded to DataFrame**
- **Given** valid .jsonl file is selected
- **When** file is read
- **Then** data successfully parsed into pandas DataFrame
- **Tests:** `test_loads_valid_jsonl_file`, `test_dataframe_has_expected_columns`

**AC4: No logs message displayed**
- **Given** logs/ directory is empty or doesn't exist
- **When** user views Results Dashboard
- **Then** helpful message with instructions displayed
- **Tests:** `test_returns_empty_list_when_directory_not_exists`, `test_returns_empty_list_when_directory_empty`

**AC5: Corrupted file error handling**
- **Given** selected log file contains invalid JSON
- **When** file is loaded
- **Then** error message displayed and app doesn't crash
- **Tests:** `test_returns_none_on_json_decode_error`, `test_handles_general_exception`

**AC6: Loading indicator shown**
- **Given** user selects a run
- **When** file is being read and parsed
- **Then** st.spinner displays loading indicator
- **Implementation:** Confirmed in `pages/2_ðŸ“Š_results_dashboard.py` line 29

### Improvements Checklist

All items completed by developer:
- [x] Excellent separation of concerns with ui_utils module
- [x] Comprehensive error handling implemented
- [x] Empty line handling in JSONL parsing
- [x] Session state cleanup on errors
- [x] 11 unit tests with 100% pass rate
- [x] Proper type hints and docstrings

### Security Review

**Status: PASS**

- âœ“ Uses `json.loads()` for safe JSON parsing (no eval or unsafe deserialization)
- âœ“ Path validation using `pathlib.Path`
- âœ“ File existence checks before access
- âœ“ Proper exception handling prevents information leakage
- âœ“ No hardcoded credentials or sensitive data

### Performance Considerations

**Status: PASS**

- âœ“ Efficient file scanning with glob pattern
- âœ“ Files sorted by modification time (reasonable for expected log volume)
- âœ“ Line-by-line JSONL parsing (memory efficient for large files)
- âœ“ pandas DataFrame creation is appropriate for data analysis
- âœ“ Session state usage minimizes redundant file reads

**Note:** For very large log files (>100MB), consider adding progress indicators or chunked loading in future iterations.

### Files Modified During Review

None - no files were modified during this review. Implementation is production-ready as submitted.

### Gate Status

Gate: **PASS** â†’ `docs/qa/gates/2.5-results-dashboard-run-selector.yml`

### Recommended Status

**âœ“ Ready for Done**

All acceptance criteria fully met, comprehensive test coverage, excellent code quality, and full standards compliance. This story is production-ready.
