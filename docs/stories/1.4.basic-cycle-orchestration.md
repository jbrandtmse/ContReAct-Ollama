# Story 1.4: Basic Cycle Orchestration

**Status**: Ready for Review

---

## Story

**As a** Researcher,
**I want** a basic `CycleOrchestrator` that can execute a defined number of empty cycles,
**so that** the fundamental structure of the experiment loop is in place.

---

## Acceptance Criteria

1. The `CycleOrchestrator` runs for the exact number of cycles defined in the `cycle_count` parameter of the config file
2. A message indicating the start and end of each cycle (e.g., "Cycle 1 starting...", "Cycle 1 finished.") is printed to the console
3. The script terminates successfully after the final cycle is completed

---

## Tasks / Subtasks

- [x] **Task 1: Create AgentState Dataclass** (AC: 1)
  - [x] Create file `contreact_ollama/state/agent_state.py`
  - [x] Define `AgentState` dataclass with all required fields
  - [x] Add type hints and field defaults
  - [x] Add docstring explaining purpose and field usage
  - [x] Import from dataclasses and typing modules

- [x] **Task 2: Create Basic CycleOrchestrator Class** (AC: 1, 2, 3)
  - [x] Create file `contreact_ollama/core/cycle_orchestrator.py`
  - [x] Implement `__init__()` method to accept config and services
  - [x] Implement `run_experiment()` method as main public entry point
  - [x] Implement `_execute_cycle()` private method for single cycle execution
  - [x] Implement `_load_state()` private method to initialize AgentState
  - [x] Add class and method docstrings following Google style

- [x] **Task 3: Implement Cycle Loop Logic** (AC: 1, 3)
  - [x] In run_experiment(), loop from cycle 1 to config.cycle_count
  - [x] Call _execute_cycle() for each iteration
  - [x] Track cycle completion
  - [x] Ensure loop terminates after cycle_count cycles

- [x] **Task 4: Add Console Output** (AC: 2)
  - [x] Print "Cycle {n} starting..." at beginning of each cycle
  - [x] Print "Cycle {n} finished." at end of each cycle
  - [x] Print experiment summary at completion
  - [x] Use clear, readable formatting

- [x] **Task 5: Integrate with ExperimentRunner** (AC: 1, 3)
  - [x] Update `ExperimentRunner.run()` method
  - [x] Create CycleOrchestrator instance with config and services
  - [x] Call run_experiment() on orchestrator
  - [x] Handle completion and exit

- [x] **Task 6: Update CLI Script** (AC: 1, 2, 3)
  - [x] Update `scripts/run_experiment.py` to call runner.run()
  - [x] Add execution confirmation messages
  - [x] Handle orchestrator completion

- [x] **Task 7: Testing** (AC: 1, 2, 3)
  - [x] Write unit tests for CycleOrchestrator
  - [x] Test cycle counting logic
  - [x] Test state initialization
  - [x] Integration test with actual config
  - [x] Manual test with sample config

---

## Dev Notes

### Previous Story Insights
From Story 1.3:
- OllamaInterface created with model verification
- ExperimentRunner.initialize_services() returns services dict with 'ollama' key
- CLI script verifies Ollama connection before proceeding

### Data Models to Implement
**Source**: [docs/architecture/data-models.md#agentstate]

Complete AgentState dataclass definition:

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any

@dataclass
class AgentState:
    """Represents the complete in-memory state of an agent at a point in time."""
    
    run_id: str  # Unique identifier for the experimental run (e.g., 'GPT5-A')
    cycle_number: int  # Current operational cycle number (e.g., 1-10)
    model_name: str  # Tag of the Ollama model being used (e.g., 'llama3:latest')
    message_history: List[Dict[str, Any]] = field(default_factory=list)  # Ordered list of all messages
    reflection_history: List[str] = field(default_factory=list)  # List of final reflection strings
```

**Message History Format** (for future use):
Each message in the `message_history` follows the Ollama chat format:
```python
{
    "role": str,  # One of: "system", "user", "assistant", "tool"
    "content": str  # The message content
}
```

### Component Specifications
**Source**: [docs/architecture/components.md#2-cycleorchestrator]

Complete CycleOrchestrator class definition:

```python
class CycleOrchestrator:
    def __init__(self, config: ExperimentConfig, ollama_interface: OllamaInterface,
                 tool_dispatcher: ToolDispatcher, logger: JsonlLogger, 
                 similarity_monitor: SimilarityMonitor):
        """Initialize orchestrator with all necessary services."""
        
    def run_experiment(self) -> None:
        """Main public method executing full experimental run from Cycle 1 to cycle_count."""
        
    def _execute_cycle(self, agent_state: AgentState) -> AgentState:
        """Execute a single cycle of the ContReAct state machine."""
        
    def _load_state(self, cycle_number: int) -> AgentState:
        """LOAD_STATE: Load or initialize AgentState."""
        
    def _assemble_prompt(self, agent_state: AgentState) -> List[Dict]:
        """ASSEMBLE_PROMPT: Construct full context for LLM."""
        
    def _invoke_llm(self, messages: List[Dict]) -> Dict:
        """INVOKE_LLM: Send prompt to Ollama server."""
        
    def _parse_response(self, response: Dict) -> Tuple[str, Any]:
        """PARSE_RESPONSE: Determine if response contains tool calls or final reflection."""
        
    def _dispatch_tool(self, tool_call: Dict, agent_state: AgentState) -> str:
        """DISPATCH_TOOL: Invoke tool and return result."""
        
    def _finalize_cycle(self, agent_state: AgentState, reflection: str) -> None:
        """FINALIZE_CYCLE: Extract reflection, calculate embedding, log state."""
        
    def _should_terminate(self, cycle_number: int) -> bool:
        """TERMINATE_OR_CONTINUE: Check if target cycle count reached."""
```

**For Story 1.4**, only implement:
- `__init__(config, ollama_interface)` - Store config and ollama interface (other services optional for now)
- `run_experiment()` - Main loop executing cycles 1 to cycle_count
- `_execute_cycle(agent_state)` - Empty implementation that just returns the state unchanged
- `_load_state(cycle_number)` - Create new AgentState with cycle number

**Do NOT implement**:
- Full state machine methods (_assemble_prompt, _invoke_llm, etc.) - those come in later stories
- Logging, tools, similarity monitoring - those come in later stories

### State Machine Workflow
**Source**: [docs/architecture/core-workflows.md#state-machine-workflow]

The 7-state ContReAct state machine:
1. **LOAD_STATE**: Load or initialize AgentState
2. **ASSEMBLE_PROMPT**: Construct full context for LLM
3. **INVOKE_LLM**: Send prompt to Ollama server
4. **PARSE_RESPONSE**: Determine if tool call or final reflection
5. **DISPATCH_TOOL**: Invoke tool and return result (conditional)
6. **FINALIZE_CYCLE**: Extract reflection, calculate embedding, log state
7. **TERMINATE_OR_CONTINUE**: Check if target cycle count reached

**For Story 1.4**, the basic structure is:
- LOAD_STATE: Create AgentState for current cycle
- Execute "empty" cycle (no LLM calls, no tools yet)
- TERMINATE_OR_CONTINUE: Check if cycle_count reached

### File Locations
**Source**: [docs/architecture/unified-project-structure.md]

Exact file paths:
- **AgentState**: `contreact_ollama/state/agent_state.py`
- **CycleOrchestrator**: `contreact_ollama/core/cycle_orchestrator.py`
- **Update**: `contreact_ollama/core/experiment_runner.py` (add run() method)
- **Update**: `scripts/run_experiment.py` (call runner.run())

### Implementation Details

**AgentState Initialization** (in _load_state):
```python
def _load_state(self, cycle_number: int) -> AgentState:
    """
    LOAD_STATE: Load or initialize AgentState.
    
    Args:
        cycle_number: Current cycle number (1-based)
        
    Returns:
        AgentState: Initialized state for this cycle
    """
    return AgentState(
        run_id=self.config.run_id,
        cycle_number=cycle_number,
        model_name=self.config.model_name,
        message_history=[],  # Empty for now, will be populated in later stories
        reflection_history=[]  # Empty for now, will be populated in later stories
    )
```

**run_experiment() Implementation**:
```python
def run_experiment(self) -> None:
    """
    Main public method executing full experimental run from Cycle 1 to cycle_count.
    
    Iterates through cycles, executing each one and tracking completion.
    """
    print(f"\nStarting experiment: {self.config.run_id}")
    print(f"Model: {self.config.model_name}")
    print(f"Total cycles: {self.config.cycle_count}\n")
    
    for cycle_num in range(1, self.config.cycle_count + 1):
        print(f"Cycle {cycle_num} starting...")
        
        # Load state for this cycle
        agent_state = self._load_state(cycle_num)
        
        # Execute cycle (empty for now)
        agent_state = self._execute_cycle(agent_state)
        
        print(f"Cycle {cycle_num} finished.")
    
    print(f"\n✓ Experiment {self.config.run_id} completed successfully")
    print(f"✓ Executed {self.config.cycle_count} cycles")
```

**_execute_cycle() Implementation** (basic version):
```python
def _execute_cycle(self, agent_state: AgentState) -> AgentState:
    """
    Execute a single cycle of the ContReAct state machine.
    
    Args:
        agent_state: Current agent state
        
    Returns:
        AgentState: Updated agent state after cycle execution
        
    Note:
        This is a basic implementation for Story 1.4.
        Full state machine logic will be added in later stories.
    """
    # For now, just return the state unchanged
    # Later stories will add: prompt assembly, LLM invocation, tool dispatch, etc.
    return agent_state
```

### ExperimentRunner.run() Implementation

Add this method to `contreact_ollama/core/experiment_runner.py`:

```python
def run(self) -> None:
    """
    Execute the complete experimental run.
    
    Orchestrates the full experiment lifecycle:
    1. Load configuration
    2. Initialize services
    3. Create and run orchestrator
    """
    # Load config (should already be done, but can be called again)
    if not hasattr(self, 'config'):
        self.config = self.load_config()
    
    # Initialize services (should already be done)
    if not hasattr(self, 'services'):
        self.services = self.initialize_services()
    
    # Create orchestrator with minimal services for now
    orchestrator = CycleOrchestrator(
        config=self.config,
        ollama_interface=self.services['ollama']
        # NOTE: logger, tools, similarity_monitor will be added in later stories
    )
    
    # Run the experiment
    orchestrator.run_experiment()
```

### CLI Script Updates

Update `scripts/run_experiment.py` to call runner.run():

```python
# ... (previous imports and setup)

try:
    # Initialize runner and load config
    runner = ExperimentRunner(args.config)
    config = runner.load_config()
    
    # Print loaded configuration
    print("Successfully loaded configuration:")
    print("-" * 50)
    print(f"Run ID: {config.run_id}")
    print(f"Model: {config.model_name}")
    print(f"Cycle Count: {config.cycle_count}")
    print(f"Ollama Host: {config.ollama_client_config.get('host')}")
    print("-" * 50)
    
    # Initialize services (includes model verification)
    print(f"\nVerifying Ollama connection and model availability...")
    services = runner.initialize_services()
    print(f"✓ Connected to Ollama server")
    print(f"✓ Model '{config.model_name}' is available")
    
    # Run the experiment
    runner.run()
    
except FileNotFoundError as e:
    print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
    print("Please check the file path and try again.", file=sys.stderr)
    sys.exit(1)
except ConnectionError as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
```

### Import Organization
**Source**: [docs/architecture/coding-standards.md#3-code-formatting]

For `contreact_ollama/state/agent_state.py`:
```python
# Standard library imports
from dataclasses import dataclass, field
from typing import List, Dict, Any

# Third-party imports
# (none for this file)

# Local application imports
# (none for this file)
```

For `contreact_ollama/core/cycle_orchestrator.py`:
```python
# Standard library imports
from typing import List, Dict, Any, Tuple, Optional

# Third-party imports
# (none for this story, will add later)

# Local application imports
from contreact_ollama.core.config import ExperimentConfig
from contreact_ollama.state.agent_state import AgentState
from contreact_ollama.llm.ollama_interface import OllamaInterface
```

For updated `contreact_ollama/core/experiment_runner.py`:
```python
# Standard library imports
from pathlib import Path
from typing import Dict, Any

# Third-party imports
import yaml

# Local application imports
from contreact_ollama.core.config import ExperimentConfig
from contreact_ollama.core.cycle_orchestrator import CycleOrchestrator
from contreact_ollama.llm.ollama_interface import OllamaInterface
```

### Coding Standards
**Source**: [docs/architecture/coding-standards.md]

**Type Hints Required**:
- All method parameters and return types
- Use `None` for run_experiment return type (it doesn't return a value)
- Use `AgentState` for state-related parameters and returns

**Docstrings Required**:
- Class-level docstring for CycleOrchestrator and AgentState
- Method docstrings for all public and private methods
- Include Args, Returns, and Note sections where applicable

**Console Output**:
- Use clear, informative messages
- Show experiment progress (cycle numbers)
- Confirm successful completion
- Use checkmarks (✓) for success messages

### Expected Console Output

When running a 3-cycle experiment:

```
Successfully loaded configuration:
--------------------------------------------------
Run ID: llama3-experiment-001
Model: llama3:latest
Cycle Count: 3
Ollama Host: http://localhost:11434
--------------------------------------------------

Verifying Ollama connection and model availability...
✓ Connected to Ollama server
✓ Model 'llama3:latest' is available

Starting experiment: llama3-experiment-001
Model: llama3:latest
Total cycles: 3

Cycle 1 starting...
Cycle 1 finished.
Cycle 2 starting...
Cycle 2 finished.
Cycle 3 starting...
Cycle 3 finished.

✓ Experiment llama3-experiment-001 completed successfully
✓ Executed 3 cycles
```

---

## Testing

**Source**: [docs/architecture/coding-standards.md#8-testing-standards]

### Test Standards for This Story
- **Test Coverage Target**: >80% code coverage
- **Test Framework**: pytest 8.2.2+
- **Test File Locations**: 
  - `tests/unit/test_cycle_orchestrator.py`
  - `tests/unit/test_agent_state.py`

### Testing Requirements for Story 1.4

**Unit Tests** (`tests/unit/test_agent_state.py`):

1. **test_agent_state_initialization**
   - Create AgentState with required fields
   - Assert all fields set correctly
   - Assert default lists are empty

2. **test_agent_state_default_factory**
   - Create two AgentState instances
   - Assert message_history lists are independent (not shared)
   - Assert reflection_history lists are independent

**Unit Tests** (`tests/unit/test_cycle_orchestrator.py`):

1. **test_init_stores_config_and_services**
   - Create CycleOrchestrator with config and ollama_interface
   - Assert config and ollama_interface stored correctly

2. **test_load_state_creates_agent_state_for_cycle**
   - Create orchestrator
   - Call _load_state(cycle_number=5)
   - Assert returned AgentState has correct cycle_number
   - Assert run_id and model_name from config

3. **test_execute_cycle_returns_state_unchanged**
   - Create AgentState
   - Call _execute_cycle(agent_state)
   - Assert returned state is the same instance or has same values

4. **test_run_experiment_executes_correct_number_of_cycles**
   - Mock _execute_cycle to track calls
   - Create config with cycle_count=5
   - Call run_experiment()
   - Assert _execute_cycle called exactly 5 times

5. **test_run_experiment_executes_cycles_in_order**
   - Mock _load_state to track cycle numbers
   - Create config with cycle_count=3
   - Call run_experiment()
   - Assert _load_state called with 1, 2, 3 in order

**Integration Tests** (`tests/integration/test_experiment_flow.py`):

1. **test_full_experiment_run_completes**
   - Create valid config file with cycle_count=2
   - Create ExperimentRunner
   - Call load_config(), initialize_services(), run()
   - Assert no exceptions raised
   - Assert experiment completes

2. **test_experiment_output_shows_cycle_messages** (using capsys):
   - Run experiment with cycle_count=2
   - Capture console output
   - Assert "Cycle 1 starting..." in output
   - Assert "Cycle 1 finished." in output
   - Assert "Cycle 2 starting..." in output
   - Assert "Cycle 2 finished." in output

**Mock Strategy Example**:
```python
from unittest.mock import Mock, MagicMock
import pytest

@pytest.fixture
def mock_config():
    config = Mock(spec=ExperimentConfig)
    config.run_id = "test-run"
    config.model_name = "llama3:latest"
    config.cycle_count = 3
    return config

@pytest.fixture
def mock_ollama_interface():
    return Mock(spec=OllamaInterface)

def test_run_experiment_executes_correct_number_of_cycles(
    mock_config, mock_ollama_interface
):
    orchestrator = CycleOrchestrator(
        config=mock_config,
        ollama_interface=mock_ollama_interface
    )
    
    # Mock _execute_cycle to track calls
    orchestrator._execute_cycle = MagicMock(
        side_effect=lambda state: state
    )
    
    # Run experiment
    orchestrator.run_experiment()
    
    # Verify _execute_cycle called 3 times (cycle_count=3)
    assert orchestrator._execute_cycle.call_count == 3
```

### Manual Testing Checklist

Before marking story complete:
- [ ] Run: `python scripts/run_experiment.py --config configs/sample-config.yaml`
- [ ] Verify console shows "Cycle 1 starting..." and "Cycle 1 finished."
- [ ] Verify all cycles execute (check against config cycle_count)
- [ ] Verify "Experiment completed successfully" message appears
- [ ] Verify experiment terminates cleanly (no errors)
- [ ] Test with different cycle_count values (1, 5, 10)
- [ ] Verify cycle numbers are correct and sequential

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (Cline)

### Debug Log References
None - Implementation completed without issues

### Completion Notes List
- Created AgentState dataclass with proper type hints and default factory for lists
- Implemented CycleOrchestrator with basic cycle execution loop
- Added run() method to ExperimentRunner for orchestration
- Updated CLI script to execute full experiment flow
- All tests passing (43/43 tests)
- Code follows project coding standards (type hints, docstrings, Google-style)
- Console output provides clear cycle progress and completion messages

### File List
**New Files:**
- `contreact_ollama/state/agent_state.py` - AgentState dataclass
- `contreact_ollama/core/cycle_orchestrator.py` - CycleOrchestrator class
- `tests/unit/test_agent_state.py` - Unit tests for AgentState
- `tests/unit/test_cycle_orchestrator.py` - Unit tests for CycleOrchestrator
- `tests/integration/test_experiment_flow.py` - Integration tests for experiment flow

**Modified Files:**
- `contreact_ollama/core/experiment_runner.py` - Added run() method and CycleOrchestrator import
- `scripts/run_experiment.py` - Updated to call runner.run()

---

## QA Results

### Review Date: 2025-01-09

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Excellent implementation with comprehensive test coverage and strong adherence to project coding standards. The story successfully delivers a basic cycle orchestration framework that will serve as the foundation for future state machine implementation.

**Strengths:**
- Clean, well-documented code with proper type hints throughout
- Comprehensive test suite (43/43 tests passing) covering unit and integration scenarios
- Proper use of dataclasses with default factory pattern for mutable defaults
- Clear separation of concerns between AgentState, CycleOrchestrator, and ExperimentRunner
- Sequential cycle execution logic is straightforward and correct
- Console output provides clear progress feedback

**Architecture Alignment:**
The implementation correctly follows the architecture specifications from docs/architecture/, including:
- AgentState dataclass matches data-models.md exactly
- CycleOrchestrator structure aligns with components.md (minimal implementation for this story)
- Proper integration with ExperimentRunner from previous story

### Refactoring Performed

No refactoring was necessary. The implementation is clean and follows best practices.

### Compliance Check

- **Coding Standards**: ✓ (with minor note below)
- **Project Structure**: ✓
- **Testing Strategy**: ✓
- **All ACs Met**: ✓

**Minor Note:** Console output uses `print()` instead of Python's `logging` module (coding-standards.md #9). However, this is acceptable for this story's scope as a demonstration output. This should be addressed when proper logging infrastructure is added in Story 1.5 (Event Logging Service).

### Requirements Traceability

All acceptance criteria have complete test coverage:

**AC1: CycleOrchestrator runs for exact number of cycles**
- **Given** a config with cycle_count=N
- **When** run_experiment() is called
- **Then** _execute_cycle() is invoked exactly N times
- **Tests:** test_run_experiment_executes_correct_number_of_cycles, test_run_experiment_with_single_cycle, test_run_experiment_with_many_cycles

**AC2: Console messages for cycle start/end**
- **Given** an experiment is running
- **When** each cycle executes
- **Then** "Cycle N starting..." and "Cycle N finished." messages are printed
- **Tests:** test_run_experiment_console_output, test_experiment_output_shows_cycle_messages

**AC3: Script terminates successfully after final cycle**
- **Given** all cycles have completed
- **When** the final cycle finishes
- **Then** experiment completes without errors and shows success message
- **Tests:** test_full_experiment_run_completes, test_experiment_with_different_cycle_counts

### Test Architecture Assessment

**Coverage:** Excellent (>80% target met)
- 6 unit tests for AgentState
- 10 unit tests for CycleOrchestrator
- 8 integration tests for experiment flow

**Test Levels:** Appropriate distribution
- Unit tests cover individual components in isolation
- Integration tests verify end-to-end flow
- Proper use of mocks to isolate dependencies

**Test Quality:**
- Clear, descriptive test names following pattern: test_<method>_<scenario>_<expected_result>
- Good use of pytest fixtures for test setup
- Proper mocking strategy to avoid external dependencies during tests
- Edge cases covered (single cycle, many cycles, config reload)

### Improvements Checklist

All items handled:
- [x] Code implementation complete and follows standards
- [x] Comprehensive test coverage achieved
- [x] Documentation (docstrings) complete
- [x] No refactoring needed

Future consideration (not blocking):
- [ ] Replace print() with logging module when Story 1.5 introduces logging infrastructure

### Security Review

**Status:** PASS

No security concerns for this story. The implementation:
- Operates on pre-validated configuration (handled in Story 1.2)
- Has no external inputs or user interaction
- Contains no authentication, authorization, or data persistence logic

### Performance Considerations

**Status:** PASS

No performance concerns:
- Simple sequential loop with O(n) complexity where n = cycle_count
- No blocking operations (empty cycle implementation for this story)
- Memory footprint minimal (AgentState with empty lists)

Future stories will add LLM invocation and tool dispatch which will introduce network I/O, but that's expected and out of scope for this story.

### Non-Functional Requirements Validation

- **Security**: PASS - No security implications
- **Performance**: PASS - Efficient implementation, no bottlenecks
- **Reliability**: PASS - Proper error handling inherited from previous stories
- **Maintainability**: PASS - Well-documented, follows project standards, easy to extend

### Files Modified During Review

None. No files were modified during this review.

### Gate Status

**Gate:** PASS → docs/qa/gates/1.4-basic-cycle-orchestration.yml

**Quality Score:** 90/100
- Minor deduction for print() vs logging (technical debt for Story 1.5)

### Recommended Status

✓ **Ready for Done**

All acceptance criteria fully met with excellent test coverage. The minor logging consideration is documented as technical debt and will be naturally addressed in Story 1.5 (Event Logging Service). No blocking issues identified.

The implementation provides a solid foundation for the full ContReAct state machine that will be built in subsequent stories.
