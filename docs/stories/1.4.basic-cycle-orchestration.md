# Story 1.4: Basic Cycle Orchestration

**Status**: Ready

---

## Story

**As a** Researcher,
**I want** a basic `CycleOrchestrator` that can execute a defined number of empty cycles,
**so that** the fundamental structure of the experiment loop is in place.

---

## Acceptance Criteria

1. The `CycleOrchestrator` runs for the exact number of cycles defined in the `cycle_count` parameter of the config file
2. A message indicating the start and end of each cycle (e.g., "Cycle 1 starting...", "Cycle 1 finished.") is printed to the console
3. The script terminates successfully after the final cycle is completed

---

## Tasks / Subtasks

- [ ] **Task 1: Create AgentState Dataclass** (AC: 1)
  - [ ] Create file `contreact_ollama/state/agent_state.py`
  - [ ] Define `AgentState` dataclass with all required fields
  - [ ] Add type hints and field defaults
  - [ ] Add docstring explaining purpose and field usage
  - [ ] Import from dataclasses and typing modules

- [ ] **Task 2: Create Basic CycleOrchestrator Class** (AC: 1, 2, 3)
  - [ ] Create file `contreact_ollama/core/cycle_orchestrator.py`
  - [ ] Implement `__init__()` method to accept config and services
  - [ ] Implement `run_experiment()` method as main public entry point
  - [ ] Implement `_execute_cycle()` private method for single cycle execution
  - [ ] Implement `_load_state()` private method to initialize AgentState
  - [ ] Add class and method docstrings following Google style

- [ ] **Task 3: Implement Cycle Loop Logic** (AC: 1, 3)
  - [ ] In run_experiment(), loop from cycle 1 to config.cycle_count
  - [ ] Call _execute_cycle() for each iteration
  - [ ] Track cycle completion
  - [ ] Ensure loop terminates after cycle_count cycles

- [ ] **Task 4: Add Console Output** (AC: 2)
  - [ ] Print "Cycle {n} starting..." at beginning of each cycle
  - [ ] Print "Cycle {n} finished." at end of each cycle
  - [ ] Print experiment summary at completion
  - [ ] Use clear, readable formatting

- [ ] **Task 5: Integrate with ExperimentRunner** (AC: 1, 3)
  - [ ] Update `ExperimentRunner.run()` method
  - [ ] Create CycleOrchestrator instance with config and services
  - [ ] Call run_experiment() on orchestrator
  - [ ] Handle completion and exit

- [ ] **Task 6: Update CLI Script** (AC: 1, 2, 3)
  - [ ] Update `scripts/run_experiment.py` to call runner.run()
  - [ ] Add execution confirmation messages
  - [ ] Handle orchestrator completion

- [ ] **Task 7: Testing** (AC: 1, 2, 3)
  - [ ] Write unit tests for CycleOrchestrator
  - [ ] Test cycle counting logic
  - [ ] Test state initialization
  - [ ] Integration test with actual config
  - [ ] Manual test with sample config

---

## Dev Notes

### Previous Story Insights
From Story 1.3:
- OllamaInterface created with model verification
- ExperimentRunner.initialize_services() returns services dict with 'ollama' key
- CLI script verifies Ollama connection before proceeding

### Data Models to Implement
**Source**: [docs/architecture/data-models.md#agentstate]

Complete AgentState dataclass definition:

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any

@dataclass
class AgentState:
    """Represents the complete in-memory state of an agent at a point in time."""
    
    run_id: str  # Unique identifier for the experimental run (e.g., 'GPT5-A')
    cycle_number: int  # Current operational cycle number (e.g., 1-10)
    model_name: str  # Tag of the Ollama model being used (e.g., 'llama3:latest')
    message_history: List[Dict[str, Any]] = field(default_factory=list)  # Ordered list of all messages
    reflection_history: List[str] = field(default_factory=list)  # List of final reflection strings
```

**Message History Format** (for future use):
Each message in the `message_history` follows the Ollama chat format:
```python
{
    "role": str,  # One of: "system", "user", "assistant", "tool"
    "content": str  # The message content
}
```

### Component Specifications
**Source**: [docs/architecture/components.md#2-cycleorchestrator]

Complete CycleOrchestrator class definition:

```python
class CycleOrchestrator:
    def __init__(self, config: ExperimentConfig, ollama_interface: OllamaInterface,
                 tool_dispatcher: ToolDispatcher, logger: JsonlLogger, 
                 similarity_monitor: SimilarityMonitor):
        """Initialize orchestrator with all necessary services."""
        
    def run_experiment(self) -> None:
        """Main public method executing full experimental run from Cycle 1 to cycle_count."""
        
    def _execute_cycle(self, agent_state: AgentState) -> AgentState:
        """Execute a single cycle of the ContReAct state machine."""
        
    def _load_state(self, cycle_number: int) -> AgentState:
        """LOAD_STATE: Load or initialize AgentState."""
        
    def _assemble_prompt(self, agent_state: AgentState) -> List[Dict]:
        """ASSEMBLE_PROMPT: Construct full context for LLM."""
        
    def _invoke_llm(self, messages: List[Dict]) -> Dict:
        """INVOKE_LLM: Send prompt to Ollama server."""
        
    def _parse_response(self, response: Dict) -> Tuple[str, Any]:
        """PARSE_RESPONSE: Determine if response contains tool calls or final reflection."""
        
    def _dispatch_tool(self, tool_call: Dict, agent_state: AgentState) -> str:
        """DISPATCH_TOOL: Invoke tool and return result."""
        
    def _finalize_cycle(self, agent_state: AgentState, reflection: str) -> None:
        """FINALIZE_CYCLE: Extract reflection, calculate embedding, log state."""
        
    def _should_terminate(self, cycle_number: int) -> bool:
        """TERMINATE_OR_CONTINUE: Check if target cycle count reached."""
```

**For Story 1.4**, only implement:
- `__init__(config, ollama_interface)` - Store config and ollama interface (other services optional for now)
- `run_experiment()` - Main loop executing cycles 1 to cycle_count
- `_execute_cycle(agent_state)` - Empty implementation that just returns the state unchanged
- `_load_state(cycle_number)` - Create new AgentState with cycle number

**Do NOT implement**:
- Full state machine methods (_assemble_prompt, _invoke_llm, etc.) - those come in later stories
- Logging, tools, similarity monitoring - those come in later stories

### State Machine Workflow
**Source**: [docs/architecture/core-workflows.md#state-machine-workflow]

The 7-state ContReAct state machine:
1. **LOAD_STATE**: Load or initialize AgentState
2. **ASSEMBLE_PROMPT**: Construct full context for LLM
3. **INVOKE_LLM**: Send prompt to Ollama server
4. **PARSE_RESPONSE**: Determine if tool call or final reflection
5. **DISPATCH_TOOL**: Invoke tool and return result (conditional)
6. **FINALIZE_CYCLE**: Extract reflection, calculate embedding, log state
7. **TERMINATE_OR_CONTINUE**: Check if target cycle count reached

**For Story 1.4**, the basic structure is:
- LOAD_STATE: Create AgentState for current cycle
- Execute "empty" cycle (no LLM calls, no tools yet)
- TERMINATE_OR_CONTINUE: Check if cycle_count reached

### File Locations
**Source**: [docs/architecture/unified-project-structure.md]

Exact file paths:
- **AgentState**: `contreact_ollama/state/agent_state.py`
- **CycleOrchestrator**: `contreact_ollama/core/cycle_orchestrator.py`
- **Update**: `contreact_ollama/core/experiment_runner.py` (add run() method)
- **Update**: `scripts/run_experiment.py` (call runner.run())

### Implementation Details

**AgentState Initialization** (in _load_state):
```python
def _load_state(self, cycle_number: int) -> AgentState:
    """
    LOAD_STATE: Load or initialize AgentState.
    
    Args:
        cycle_number: Current cycle number (1-based)
        
    Returns:
        AgentState: Initialized state for this cycle
    """
    return AgentState(
        run_id=self.config.run_id,
        cycle_number=cycle_number,
        model_name=self.config.model_name,
        message_history=[],  # Empty for now, will be populated in later stories
        reflection_history=[]  # Empty for now, will be populated in later stories
    )
```

**run_experiment() Implementation**:
```python
def run_experiment(self) -> None:
    """
    Main public method executing full experimental run from Cycle 1 to cycle_count.
    
    Iterates through cycles, executing each one and tracking completion.
    """
    print(f"\nStarting experiment: {self.config.run_id}")
    print(f"Model: {self.config.model_name}")
    print(f"Total cycles: {self.config.cycle_count}\n")
    
    for cycle_num in range(1, self.config.cycle_count + 1):
        print(f"Cycle {cycle_num} starting...")
        
        # Load state for this cycle
        agent_state = self._load_state(cycle_num)
        
        # Execute cycle (empty for now)
        agent_state = self._execute_cycle(agent_state)
        
        print(f"Cycle {cycle_num} finished.")
    
    print(f"\n✓ Experiment {self.config.run_id} completed successfully")
    print(f"✓ Executed {self.config.cycle_count} cycles")
```

**_execute_cycle() Implementation** (basic version):
```python
def _execute_cycle(self, agent_state: AgentState) -> AgentState:
    """
    Execute a single cycle of the ContReAct state machine.
    
    Args:
        agent_state: Current agent state
        
    Returns:
        AgentState: Updated agent state after cycle execution
        
    Note:
        This is a basic implementation for Story 1.4.
        Full state machine logic will be added in later stories.
    """
    # For now, just return the state unchanged
    # Later stories will add: prompt assembly, LLM invocation, tool dispatch, etc.
    return agent_state
```

### ExperimentRunner.run() Implementation

Add this method to `contreact_ollama/core/experiment_runner.py`:

```python
def run(self) -> None:
    """
    Execute the complete experimental run.
    
    Orchestrates the full experiment lifecycle:
    1. Load configuration
    2. Initialize services
    3. Create and run orchestrator
    """
    # Load config (should already be done, but can be called again)
    if not hasattr(self, 'config'):
        self.config = self.load_config()
    
    # Initialize services (should already be done)
    if not hasattr(self, 'services'):
        self.services = self.initialize_services()
    
    # Create orchestrator with minimal services for now
    orchestrator = CycleOrchestrator(
        config=self.config,
        ollama_interface=self.services['ollama']
        # NOTE: logger, tools, similarity_monitor will be added in later stories
    )
    
    # Run the experiment
    orchestrator.run_experiment()
```

### CLI Script Updates

Update `scripts/run_experiment.py` to call runner.run():

```python
# ... (previous imports and setup)

try:
    # Initialize runner and load config
    runner = ExperimentRunner(args.config)
    config = runner.load_config()
    
    # Print loaded configuration
    print("Successfully loaded configuration:")
    print("-" * 50)
    print(f"Run ID: {config.run_id}")
    print(f"Model: {config.model_name}")
    print(f"Cycle Count: {config.cycle_count}")
    print(f"Ollama Host: {config.ollama_client_config.get('host')}")
    print("-" * 50)
    
    # Initialize services (includes model verification)
    print(f"\nVerifying Ollama connection and model availability...")
    services = runner.initialize_services()
    print(f"✓ Connected to Ollama server")
    print(f"✓ Model '{config.model_name}' is available")
    
    # Run the experiment
    runner.run()
    
except FileNotFoundError as e:
    print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
    print("Please check the file path and try again.", file=sys.stderr)
    sys.exit(1)
except ConnectionError as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
```

### Import Organization
**Source**: [docs/architecture/coding-standards.md#3-code-formatting]

For `contreact_ollama/state/agent_state.py`:
```python
# Standard library imports
from dataclasses import dataclass, field
from typing import List, Dict, Any

# Third-party imports
# (none for this file)

# Local application imports
# (none for this file)
```

For `contreact_ollama/core/cycle_orchestrator.py`:
```python
# Standard library imports
from typing import List, Dict, Any, Tuple, Optional

# Third-party imports
# (none for this story, will add later)

# Local application imports
from contreact_ollama.core.config import ExperimentConfig
from contreact_ollama.state.agent_state import AgentState
from contreact_ollama.llm.ollama_interface import OllamaInterface
```

For updated `contreact_ollama/core/experiment_runner.py`:
```python
# Standard library imports
from pathlib import Path
from typing import Dict, Any

# Third-party imports
import yaml

# Local application imports
from contreact_ollama.core.config import ExperimentConfig
from contreact_ollama.core.cycle_orchestrator import CycleOrchestrator
from contreact_ollama.llm.ollama_interface import OllamaInterface
```

### Coding Standards
**Source**: [docs/architecture/coding-standards.md]

**Type Hints Required**:
- All method parameters and return types
- Use `None` for run_experiment return type (it doesn't return a value)
- Use `AgentState` for state-related parameters and returns

**Docstrings Required**:
- Class-level docstring for CycleOrchestrator and AgentState
- Method docstrings for all public and private methods
- Include Args, Returns, and Note sections where applicable

**Console Output**:
- Use clear, informative messages
- Show experiment progress (cycle numbers)
- Confirm successful completion
- Use checkmarks (✓) for success messages

### Expected Console Output

When running a 3-cycle experiment:

```
Successfully loaded configuration:
--------------------------------------------------
Run ID: llama3-experiment-001
Model: llama3:latest
Cycle Count: 3
Ollama Host: http://localhost:11434
--------------------------------------------------

Verifying Ollama connection and model availability...
✓ Connected to Ollama server
✓ Model 'llama3:latest' is available

Starting experiment: llama3-experiment-001
Model: llama3:latest
Total cycles: 3

Cycle 1 starting...
Cycle 1 finished.
Cycle 2 starting...
Cycle 2 finished.
Cycle 3 starting...
Cycle 3 finished.

✓ Experiment llama3-experiment-001 completed successfully
✓ Executed 3 cycles
```

---

## Testing

**Source**: [docs/architecture/coding-standards.md#8-testing-standards]

### Test Standards for This Story
- **Test Coverage Target**: >80% code coverage
- **Test Framework**: pytest 8.2.2+
- **Test File Locations**: 
  - `tests/unit/test_cycle_orchestrator.py`
  - `tests/unit/test_agent_state.py`

### Testing Requirements for Story 1.4

**Unit Tests** (`tests/unit/test_agent_state.py`):

1. **test_agent_state_initialization**
   - Create AgentState with required fields
   - Assert all fields set correctly
   - Assert default lists are empty

2. **test_agent_state_default_factory**
   - Create two AgentState instances
   - Assert message_history lists are independent (not shared)
   - Assert reflection_history lists are independent

**Unit Tests** (`tests/unit/test_cycle_orchestrator.py`):

1. **test_init_stores_config_and_services**
   - Create CycleOrchestrator with config and ollama_interface
   - Assert config and ollama_interface stored correctly

2. **test_load_state_creates_agent_state_for_cycle**
   - Create orchestrator
   - Call _load_state(cycle_number=5)
   - Assert returned AgentState has correct cycle_number
   - Assert run_id and model_name from config

3. **test_execute_cycle_returns_state_unchanged**
   - Create AgentState
   - Call _execute_cycle(agent_state)
   - Assert returned state is the same instance or has same values

4. **test_run_experiment_executes_correct_number_of_cycles**
   - Mock _execute_cycle to track calls
   - Create config with cycle_count=5
   - Call run_experiment()
   - Assert _execute_cycle called exactly 5 times

5. **test_run_experiment_executes_cycles_in_order**
   - Mock _load_state to track cycle numbers
   - Create config with cycle_count=3
   - Call run_experiment()
   - Assert _load_state called with 1, 2, 3 in order

**Integration Tests** (`tests/integration/test_experiment_flow.py`):

1. **test_full_experiment_run_completes**
   - Create valid config file with cycle_count=2
   - Create ExperimentRunner
   - Call load_config(), initialize_services(), run()
   - Assert no exceptions raised
   - Assert experiment completes

2. **test_experiment_output_shows_cycle_messages** (using capsys):
   - Run experiment with cycle_count=2
   - Capture console output
   - Assert "Cycle 1 starting..." in output
   - Assert "Cycle 1 finished." in output
   - Assert "Cycle 2 starting..." in output
   - Assert "Cycle 2 finished." in output

**Mock Strategy Example**:
```python
from unittest.mock import Mock, MagicMock
import pytest

@pytest.fixture
def mock_config():
    config = Mock(spec=ExperimentConfig)
    config.run_id = "test-run"
    config.model_name = "llama3:latest"
    config.cycle_count = 3
    return config

@pytest.fixture
def mock_ollama_interface():
    return Mock(spec=OllamaInterface)

def test_run_experiment_executes_correct_number_of_cycles(
    mock_config, mock_ollama_interface
):
    orchestrator = CycleOrchestrator(
        config=mock_config,
        ollama_interface=mock_ollama_interface
    )
    
    # Mock _execute_cycle to track calls
    orchestrator._execute_cycle = MagicMock(
        side_effect=lambda state: state
    )
    
    # Run experiment
    orchestrator.run_experiment()
    
    # Verify _execute_cycle called 3 times (cycle_count=3)
    assert orchestrator._execute_cycle.call_count == 3
```

### Manual Testing Checklist

Before marking story complete:
- [ ] Run: `python scripts/run_experiment.py --config configs/sample-config.yaml`
- [ ] Verify console shows "Cycle 1 starting..." and "Cycle 1 finished."
- [ ] Verify all cycles execute (check against config cycle_count)
- [ ] Verify "Experiment completed successfully" message appears
- [ ] Verify experiment terminates cleanly (no errors)
- [ ] Test with different cycle_count values (1, 5, 10)
- [ ] Verify cycle numbers are correct and sequential

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
