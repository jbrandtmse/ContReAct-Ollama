# Story 1.3: Ollama Connection and Model Verification

**Status**: Ready

---

## Story

**As a** Researcher,
**I want** the application to connect to my local Ollama server and verify the required model is available,
**so that** I can ensure my experiment will run with the correct LLM.

---

## Acceptance Criteria

1. The script initializes an `OllamaInterface` that connects to the Ollama host specified in the config file
2. The `OllamaInterface` successfully retrieves the list of locally available models from the Ollama server
3. If the `model_name` from the config file is present in the list of available models, the script proceeds
4. If the `model_name` is not found, the script terminates with a clear error message instructing the user to run `ollama pull <model_name>`

---

## Tasks / Subtasks

- [ ] **Task 1: Create OllamaInterface Class** (AC: 1, 2)
  - [ ] Create file `contreact_ollama/llm/ollama_interface.py`
  - [ ] Import ollama library
  - [ ] Implement `__init__(self, host: str)` method to initialize ollama.Client
  - [ ] Add class docstring with purpose and usage examples
  - [ ] Add type hints for all parameters

- [ ] **Task 2: Implement Model Availability Verification** (AC: 2, 3, 4)
  - [ ] Implement `verify_model_availability(self, model_name: str) -> bool` method
  - [ ] Call `ollama.list()` to retrieve available models
  - [ ] Parse response to extract model names
  - [ ] Check if requested model_name is in available models list
  - [ ] Return True if found, raise exception if not found
  - [ ] Add comprehensive docstring with Args, Returns, Raises sections

- [ ] **Task 3: Implement Error Handling** (AC: 4)
  - [ ] Define custom exception `ModelNotFoundError` (or use ValueError)
  - [ ] Handle `ollama.ResponseError` for connection failures
  - [ ] Provide clear error message with `ollama pull <model_name>` instruction
  - [ ] Handle network errors and timeout scenarios

- [ ] **Task 4: Integrate with ExperimentRunner** (AC: 1, 3, 4)
  - [ ] Update `ExperimentRunner.initialize_services()` method
  - [ ] Create OllamaInterface instance with host from config
  - [ ] Call verify_model_availability() with model_name from config
  - [ ] Handle exceptions and exit gracefully with error message

- [ ] **Task 5: Update CLI Script** (AC: 1, 3, 4)
  - [ ] Update `scripts/run_experiment.py` to call initialize_services()
  - [ ] Add Ollama connection verification after config loading
  - [ ] Print success message when model is verified
  - [ ] Handle and display Ollama-related errors

- [ ] **Task 6: Testing** (AC: 1, 2, 3, 4)
  - [ ] Write unit tests for OllamaInterface
  - [ ] Mock ollama.Client for testing
  - [ ] Test successful model verification
  - [ ] Test model not found scenario
  - [ ] Test connection error scenarios
  - [ ] Manual test with actual Ollama server

---

## Dev Notes

### Previous Story Insights
From Story 1.2:
- ExperimentRunner class created with load_config() method
- ExperimentConfig dataclass available with ollama_client_config field
- CLI script `scripts/run_experiment.py` loads configuration

### Component Specifications
**Source**: [docs/architecture/components.md#5-ollamainterface]

Complete OllamaInterface class definition:

```python
class OllamaInterface:
    def __init__(self, host: str = "http://localhost:11434"):
        """Initialize Ollama client with specified host."""
        self.client = ollama.Client(host=host)
        
    def verify_model_availability(self, model_name: str) -> bool:
        """
        Verify model exists locally.
        
        Calls ollama.list() and checks if model_name is present.
        Raises error with instructions to run 'ollama pull' if not found.
        """
        
    def execute_chat_completion(self, model_name: str, messages: List[Dict], 
                                tools: List[Dict], options: Dict) -> Dict:
        """
        Execute LLM chat completion.
        
        Args:
            model_name: Tag of model to use
            messages: Message history
            tools: Tool definitions in JSON schema format
            options: Generation parameters (temperature, seed, etc.)
            
        Returns:
            Response object from ollama.chat
            
        Raises:
            ollama.ResponseError: On connection or model errors
        """
```

**For Story 1.3**, only implement:
- `__init__(self, host: str)` - Initialize ollama.Client
- `verify_model_availability(self, model_name: str) -> bool` - Check model exists

Do NOT implement `execute_chat_completion()` yet - that comes in Story 1.8.

### File Locations
**Source**: [docs/architecture/unified-project-structure.md]

Exact file paths:
- **OllamaInterface**: `contreact_ollama/llm/ollama_interface.py`
- **Update**: `contreact_ollama/core/experiment_runner.py` (add initialize_services method)
- **Update**: `scripts/run_experiment.py` (add model verification step)

### Ollama Client Configuration
**Source**: [docs/architecture/data-models.md#experimentconfig]

From ExperimentConfig, the ollama_client_config structure:
```python
ollama_client_config: Dict[str, Any]  # Configuration for Ollama client
```

Example from sample config:
```yaml
ollama_client_config:
  host: "http://localhost:11434"  # Ollama server URL
```

**Accessing in code**:
```python
host = config.ollama_client_config.get('host', 'http://localhost:11434')
ollama_interface = OllamaInterface(host=host)
```

### Ollama API Integration
**Source**: [docs/architecture/external-apis.md]

The ollama Python library provides:
- `ollama.Client(host)` - Initialize client with server URL
- `client.list()` - Returns list of available models
- `client.chat()` - Execute chat completions (not needed for this story)

**ollama.list() Response Format**:
```python
{
    'models': [
        {
            'name': 'llama3:latest',
            'modified_at': '2024-01-01T00:00:00Z',
            'size': 4661224384,
            # ... other fields
        },
        # ... more models
    ]
}
```

**Model Name Matching**:
- Extract model names from response['models']
- Each model has a 'name' field (e.g., 'llama3:latest')
- Compare config model_name against these names
- Match must be exact (case-sensitive)

### Error Handling Requirements
**Source**: [docs/architecture/coding-standards.md#5-error-handling]

Specific exceptions to handle:

1. **ollama.ResponseError**: Ollama server connection failed
   - Message: `"Error: Failed to connect to Ollama server at {host}"`
   - Suggestion: `"Please ensure Ollama is running. Start with: 'ollama serve'"`

2. **Model Not Found**: Requested model not in available models
   - Message: `"Error: Model '{model_name}' not found locally"`
   - Suggestion: `"Please pull the model first: ollama pull {model_name}"`

3. **Connection/Network Errors**: General network issues
   - Message: `"Error: Network error connecting to Ollama: {error_details}"`
   - Suggestion: `"Check your network connection and Ollama server status."`

**Custom Exception** (optional):
```python
class ModelNotFoundError(Exception):
    """Raised when required model is not available locally."""
    pass
```

### Implementation Details

**verify_model_availability() Logic**:
```python
def verify_model_availability(self, model_name: str) -> bool:
    """
    Verify model exists locally.
    
    Args:
        model_name: Tag of model to verify (e.g., 'llama3:latest')
        
    Returns:
        True if model is available locally
        
    Raises:
        ollama.ResponseError: If connection to Ollama fails
        ModelNotFoundError: If model not found locally
        
    Example:
        >>> interface = OllamaInterface()
        >>> interface.verify_model_availability('llama3:latest')
        True
    """
    try:
        # Get list of available models
        response = self.client.list()
        available_models = [model['name'] for model in response.get('models', [])]
        
        # Check if requested model is available
        if model_name in available_models:
            return True
        else:
            raise ModelNotFoundError(
                f"Model '{model_name}' not found locally. "
                f"Please pull the model first: ollama pull {model_name}"
            )
            
    except ollama.ResponseError as e:
        raise ConnectionError(
            f"Failed to connect to Ollama server: {e}. "
            "Please ensure Ollama is running with: ollama serve"
        )
```

### ExperimentRunner.initialize_services() Implementation

Add this method to `contreact_ollama/core/experiment_runner.py`:

```python
def initialize_services(self) -> dict:
    """
    Initialize all required services (Ollama, Logger, Tools, etc.).
    
    Returns:
        Dictionary containing initialized service instances
        
    Raises:
        ConnectionError: If Ollama connection fails
        ModelNotFoundError: If model not available
    """
    services = {}
    
    # Initialize Ollama interface
    host = self.config.ollama_client_config.get('host', 'http://localhost:11434')
    ollama_interface = OllamaInterface(host=host)
    
    # Verify model availability
    ollama_interface.verify_model_availability(self.config.model_name)
    
    services['ollama'] = ollama_interface
    
    # NOTE: Other services (Logger, Tools, etc.) will be added in later stories
    
    return services
```

### CLI Script Updates

Update `scripts/run_experiment.py`:

```python
#!/usr/bin/env python3
"""
ContReAct-Ollama Experiment Runner

Executes experimental runs using local Ollama LLM models.

Usage:
    python scripts/run_experiment.py --config configs/sample-config.yaml
"""

# Standard library imports
import argparse
import sys
from pathlib import Path

# Local application imports
from contreact_ollama.core.experiment_runner import ExperimentRunner

def main():
    parser = argparse.ArgumentParser(
        description='Run ContReAct-Ollama experimental session'
    )
    parser.add_argument(
        '--config',
        required=True,
        help='Path to YAML configuration file'
    )
    args = parser.parse_args()
    
    try:
        # Initialize runner and load config
        runner = ExperimentRunner(args.config)
        config = runner.load_config()
        
        # Print loaded configuration
        print("Successfully loaded configuration:")
        print("-" * 50)
        print(f"Run ID: {config.run_id}")
        print(f"Model: {config.model_name}")
        print(f"Cycle Count: {config.cycle_count}")
        print(f"Ollama Host: {config.ollama_client_config.get('host')}")
        print("-" * 50)
        
        # Initialize services (includes model verification)
        print(f"\nVerifying Ollama connection and model availability...")
        services = runner.initialize_services()
        print(f"✓ Connected to Ollama server")
        print(f"✓ Model '{config.model_name}' is available")
        
        # Future: runner.run() will execute experiment
        
    except FileNotFoundError as e:
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        print("Please check the file path and try again.", file=sys.stderr)
        sys.exit(1)
    except ConnectionError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
```

### Import Organization
**Source**: [docs/architecture/coding-standards.md#3-code-formatting]

For `contreact_ollama/llm/ollama_interface.py`:
```python
# Standard library imports
from typing import List, Dict, Any

# Third-party imports
import ollama

# Local application imports
# (none for this file)
```

For updated `contreact_ollama/core/experiment_runner.py`:
```python
# Standard library imports
from pathlib import Path
from typing import Dict, Any

# Third-party imports
import yaml

# Local application imports
from contreact_ollama.core.config import ExperimentConfig
from contreact_ollama.llm.ollama_interface import OllamaInterface
```

### Coding Standards
**Source**: [docs/architecture/coding-standards.md]

**Type Hints Required**:
- All method parameters and return types
- Use `bool` for verify_model_availability return
- Use `str` for host and model_name parameters

**Docstrings Required**:
- Class-level docstring for OllamaInterface
- Method docstrings for __init__ and verify_model_availability
- Include Args, Returns, Raises, and Example sections

**Error Messages**:
- Must be clear and actionable
- Include specific instructions (e.g., "ollama pull <model_name>")
- Use proper exception types, never bare except

---

## Testing

**Source**: [docs/architecture/coding-standards.md#8-testing-standards]

### Test Standards for This Story
- **Test Coverage Target**: >80% code coverage
- **Test Framework**: pytest 8.2.2+
- **Test File Location**: `tests/unit/test_ollama_interface.py`

### Testing Requirements for Story 1.3

**Unit Tests** (`tests/unit/test_ollama_interface.py`):

1. **test_init_creates_client_with_default_host**
   - Create OllamaInterface without host parameter
   - Assert client initialized with default localhost:11434

2. **test_init_creates_client_with_custom_host**
   - Create OllamaInterface with custom host
   - Assert client initialized with specified host

3. **test_verify_model_availability_model_exists_returns_true**
   - Mock ollama.Client.list() to return sample models
   - Call verify_model_availability with available model
   - Assert returns True

4. **test_verify_model_availability_model_not_found_raises_error**
   - Mock ollama.Client.list() to return sample models (not including test model)
   - Call verify_model_availability with unavailable model
   - Assert raises ModelNotFoundError with helpful message

5. **test_verify_model_availability_connection_error_raises_error**
   - Mock ollama.Client.list() to raise ollama.ResponseError
   - Call verify_model_availability
   - Assert raises ConnectionError with helpful message

**Integration Tests** (`tests/integration/test_experiment_runner.py`):

1. **test_initialize_services_with_valid_model**
   - Requires actual Ollama running with test model
   - Create ExperimentRunner with valid config
   - Call initialize_services()
   - Assert services dict contains ollama interface
   - Assert no exceptions raised

2. **test_initialize_services_with_invalid_model_fails**
   - Requires actual Ollama running
   - Create config with non-existent model
   - Call initialize_services()
   - Assert raises ModelNotFoundError

**Mock Strategy**:
```python
from unittest.mock import Mock, patch
import pytest

@pytest.fixture
def mock_ollama_client():
    with patch('ollama.Client') as mock:
        yield mock

def test_verify_model_availability_model_exists_returns_true(mock_ollama_client):
    # Setup mock
    mock_instance = Mock()
    mock_instance.list.return_value = {
        'models': [
            {'name': 'llama3:latest'},
            {'name': 'mistral:latest'}
        ]
    }
    mock_ollama_client.return_value = mock_instance
    
    # Test
    interface = OllamaInterface()
    result = interface.verify_model_availability('llama3:latest')
    
    assert result == True
```

### Manual Testing Checklist

Before marking story complete:
- [ ] Ensure Ollama is running: `ollama serve`
- [ ] Pull test model: `ollama pull llama3:latest`
- [ ] Run: `python scripts/run_experiment.py --config configs/sample-config.yaml`
- [ ] Verify output shows "✓ Model 'llama3:latest' is available"
- [ ] Stop Ollama service
- [ ] Run script again, verify connection error message is clear
- [ ] Restart Ollama
- [ ] Update config to use non-existent model
- [ ] Run script, verify "ollama pull" instruction appears

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-08 | 1.0 | Initial story creation | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
*To be populated by dev agent during implementation*

### Debug Log References
*To be populated by dev agent during implementation*

### Completion Notes List
*To be populated by dev agent during implementation*

### File List
*To be populated by dev agent during implementation*

---

## QA Results
*To be populated by QA agent after implementation*
